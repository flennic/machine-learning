---
title: "Ensemble Methods and Mixture Models"
author: "Maximilian Pfundstein (maxpf364)"
date: "27 November 2018"
#bibliography: sources.bib
output:
  pdf_document:
    toc: true
    toc_depth: 3
  html_document:
    df_print: paged
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mboost)
library(randomForest)
library(ggplot2)
library(knitr)
set.seed(1234567890)
```

# 1 Ensemble Methods

Let's load the dataset and have a look at it. The dataset is truncated to only show the last 10 columns.

```{r, echo=FALSE}
spambase = read.csv("spambase.csv", sep=";", dec = ",")
spambase$Spam = as.factor(spambase$Spam)

n = dim(spambase)[1]
id = sample(1:n, floor(n*0.67))
train_spambase = spambase[id,]
val_spambase = spambase[-id,]

kable(head(spambase[,48:58]), caption = "spambase.csv")
```

The following source code calls the Random Forest and AdaBoost implementation and uses the predict function of each for getting the error rates for the training and the validation data set. The functions are called for 10, 20, ..., 100 trees.

```{r, echo=TRUE}

# General Information
c_formula = Spam ~ .
tree_sizes = seq(from = 10, to = 100, by = 10)

# Random Forest
rf_errors = data.frame(n = numeric(), error_rate_training = numeric(),
                       error_rate_validation = numeric())

for (i in tree_sizes) {
  
  # Create the forest
  c_randomForest =
    randomForest(formula = c_formula, data = train_spambase, ntree = i)
  
  # Do the prediction on the validation dataset
  c_prediction_training =
    predict(object = c_randomForest, newdata = train_spambase)
  c_prediction_validation =
    predict(object = c_randomForest, newdata = val_spambase)
  
  # Get the error rate
  c_error_rate_training = 1 - sum(c_prediction_training ==
                                    train_spambase$Spam)/nrow(train_spambase)
  c_error_rate_validation = 1 - sum(c_prediction_validation ==
                                      val_spambase$Spam)/nrow(val_spambase)
  
  rf_errors = rbind(rf_errors,
                    list(n = i,
                         error_rate_training = c_error_rate_training,
                         error_rate_validation = c_error_rate_validation))
}

# AdaBoost
adb_errors = data.frame(n = numeric(), error_rate_training = numeric(),
                       error_rate_validation = numeric())

for (i in tree_sizes) {

  # Create the model
  c_adaBoost = blackboost(formula = c_formula,
                          data = train_spambase,
                          family = AdaExp(),
                          control=boost_control(mstop=i))
  
  # Do the prediction on the validation dataset
  c_prediction_training =
    predict(object = c_adaBoost, newdata = train_spambase, type = "class")
  c_prediction_validation =
    predict(object = c_adaBoost, newdata = val_spambase, type = "class")
  
  # Get the error rate
  c_error_rate_training = 1 - sum(c_prediction_training ==
                                    train_spambase$Spam)/nrow(train_spambase)
  c_error_rate_validation = 1 - sum(c_prediction_validation ==
                                      val_spambase$Spam)/nrow(val_spambase)
  
  adb_errors = rbind(adb_errors,
                    list(n = i, error_rate_training = c_error_rate_training,
                         error_rate_validation = c_error_rate_validation))
}

```

The following tables show the error rates for Random Forst and AdaBoost. The plot visualizes this data, the dashed lines represent the performance on the training data set.

```{r, echo=FALSE}

kable(rf_errors, caption = "Error rates for Random Forest")
kable(adb_errors, caption = "Error rates for AdaBoost")

ggplot(adb_errors) +
  geom_line(aes(x = n, y = error_rate_training,
                colour = "AdaBoost Training"), linetype = "dashed") +
  geom_point(aes(x = n, y = error_rate_training), colour = "orange") +
  
  geom_line(aes(x = n, y = error_rate_validation,
                colour = "AdaBoost Validation")) +
  geom_point(aes(x = n, y = error_rate_validation), colour = "red") +
  
  geom_line(aes(x = n, y = error_rate_training,
                colour = "Random Forest Training"),
            data = rf_errors, linetype = "dashed") +
  geom_point(aes(x = n, y = error_rate_training),
             colour = "blue", data = rf_errors) +
  
  geom_line(aes(x = n, y = error_rate_validation,
                colour = "Random Forest Validation"), data = rf_errors) +
  geom_point(aes(x = n, y = error_rate_validation),
             colour = "steelblue2", data = rf_errors) +
  labs(title = "Random Forest and AdaBoost", y = "Error Rate",
       x = "Number of Forests", color = "Legend") +
  scale_color_manual(values = c("orange", "red", "blue", "steelblue2"))

```

We can observe that the Random Forest is performing way better than AdaBoost. Still AdaBoost seems to perform way better on the training with respect to the validation data set than compared to the Random Forest which has a big gap between the training and validation data set. The only thing which seems to be weird is that Adaboost is actually performing better on the validation data set compared to the training data set. This behavior changes when the seed is changed, so it might be just an occasion.


# 2 Mixture Models
For the EM-algorithm one must calculate the $Z$ matrix, the likelihood $L$ and the new $\mu$ and $\pi$ for each iteration. These can be done with matrix multiplications and thus in the followingit will be explained which formulas were used and what the values $Z$, $L$, $\mu$, $\pi$ and $X$ mean.

We have $X$ which holds $N = 1000$ coin tosses where the coin was tossed $D$ times. In the real world we will not know from how many components our data is derived nor how many entries belong to each component.

In this example $\pi$ holds our estimate how many datapoints from $X$ belong to $K_i$ (which is $1/3$ for each, but let's suppose we don't know.)

$\mu$ represents the probability for each $D$ to be head or tail for each $K_i$.

The likelihood $L$is basically the expected value of our parameters that our model derived from the data. The goal is to maximize tis value iteratively. Due to [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality) it can be proofed that the likelihood is always greater or equal in the next iteration step of the EM-algorithm [Gently Building Up the EM Algorithm](https://abidlabs.github.io/EM-Algorithm/#mjx-eqn-eq5). Thus we will at least not get worste with each iteration, we shold just care if the likelihood doesn't improved much with an itration and then stop. The $min_change$ is set to $0.1$ (note that we speak about the ln-likelihood).



# Appendix

```{r, ref.label = knitr::all_labels(), echo = TRUE, eval = FALSE}
```

# Bibliography