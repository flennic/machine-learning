---
title: "Lab 02 Block 02 | Machine Learning - LiU"
author: "Annalena Erhard (anner218), HÃ©ctor Plata (hecpl268), Maximilian Pfundstein (maxpf364)"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    toc_depth: 3
  html_document:
    df_print: paged
    toc_float: true
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
library(readxl)
library(ggplot2)
library(mgcv)
library(pamr)
library(knitr)
library(glmnet)
library(kernlab)

```

# Assignment 1

The excel document **influenza.xlsx** contians weekly data on the mortality and the number of laboratory-confirmed cases of influenza in Sweden. In addition, there is information about population-weighted temperature anomalies (temperature deficits).

## Task 1.1

**Task:** Use time series plots to visually inspect how the mortality and influenza number vary with time (use Time as X axis). By using this plot, comment how the amounts of influenza cases are related to mortality rates.

**Answer:** From the plots shown below we see that the peaks of influenza cases are positioned at the same time as the peaks for moratlity rates. Which could possible mean in this case that the number of influenza cases drives the mortality for this periods.

```{r}
############
# TASK 1.1 #
############

# Loading the data and preprocessing.
data = read_xlsx("Influenza.xlsx")
data$Date = as.Date(paste(data$Year, data$Week, 1, sep="-"), "%Y-%U-%u")


# Plotting the time dependancy of
# influenza and moratlity rate.
p = ggplot() +
    geom_line(aes(x=data$Date, y=data$Mortality)) +
    labs(x='Date',
         y='Mortality',
         title='Mortality over the years')

print(p)

p = ggplot() +
    geom_line(aes(x=data$Date, y=data$Influenza)) +
    labs(x='Date',
         y='Influenza',
         title='Influenza over the years')

print(p)

ggplot(data) +
  geom_line(aes(x = data$Time, y = data$Mortality,
                colour = "Mortality Time Series")) +
  geom_line(aes(x = data$Time, y = data$Influenza,
                colour = "Influenza Time Series")) +
  labs(title = "Superimposed Time Series", y = "Values",
       x = "Time", color = "Legend") +
  scale_color_manual(values = c("#604dc5","#f83d69"))

```

## Task 1.2

**Task:** Use `gam()` function from `mgcv` package to fit a `GAM` model in which `Mortality` is normally distributed and modelled as a linear function of `Year` and spline function of `Week`, and make sure that the model parameters are selected by the generalized cross-validation. Report the underlying probabilistic model.

**Answer:** The probabilistic model can be seen below with the code for fitting the model.

$$Mortality \sim \mathcal{N}\left(\alpha + X_{year}\beta_{year} + f_{week}\left(X_{week}\right), \sigma^2\right)$$

Where,

$$f_{week}\left(X_{week}\right) = \phi_{week}\left(X_{week}\right)\beta_{week}$$

```{r echo=TRUE, message=FALSE}
############
# TASK 1.2 #
############

# Fitting the model.
reg = gam(Mortality ~ Year + s(Week, k=length(unique(data$Week))), 
          data=data)

```

```{r}

summary(reg)
plot(reg)

```


## Task 1.3

**Task:** Plot predicted and observed mortality against time for the fitted model and comment on the quality of the fit. Investigate the output of the `GAM` model and report which terms appear to be significant in the model. Is there a trend in mortality change from one year to another? Plot the spline component and interpret the plot.

**Answer:** It looks like the model is able to capture all the trends form the original data while avoiding the noise from the time series. From the looks of it, the model is a good fit for the data. The only term that is significant from the summary with a significance level of $1\%$ is $\beta_{week}$ which corresponds to the coefficient of the spline function. There doesn't seem to be a change in trend of mortality by year, since the plot doesn't show a change of behaviour and the linear coefficient for `Year` is not significant.

Below is presented the plot of the spline component and the mortality by `Week` for different values of `Year`. We see that the spline component mimics the average behaviour of the mortality given the `Week` by `Year`. This shows that the mortality usually have a peak around the begining and end of the year while it decreases at the middle of it.

The influence of the smoothed variable  week  behaves almost like a parabola and mimics the course of the data over one year.

```{r}
############
# TASK 1.3 #
############

# Getting the fitted values and
# creating a plot of 
# y vs yhat.
y_hat = fitted(reg)

# Plotting the real values and the fitted ones.
p = ggplot() +
    geom_line(aes(x=data$Date, y=data$Mortality, color='True values')) +
    geom_line(aes(x=data$Date, y=y_hat, color='Fitted values')) +
    scale_colour_manual(values=c("#604dc5","#f83d69")) +
    labs(x='Time',
         y='Mortality',
         title='True values and predicted values of Mortality')

print(p)

# Output of the regression object.
summary(reg)

# Plotting the spline component.
plot(reg)

# Plotting the mortality by week and by year.
p = ggplot() +
    geom_line(aes(x=data$Week,
                  y=data$Mortality,
                  colour=as.factor(data$Year))) +
    labs(x='Week',
         y='Mortality',
         colour='Year',
         title='Mortality by Week and by Year')

print(p)

```

## Task 1.4

**Task:** Examine how the penalty factor of the spline function in the `GAM` model from step $2$ influences the estimated deviance of the model. Make plots of the predicted and observed mortality against time for cases of very high and very low penalty factors. What is the relation of the penalty factor to the degrees of freedom? Do your results confirm this relationship?

**Answer:** We see that the higher the $\lambda$ factor the bigger the deviance of the model, which is expected as it's making the model less complex and it increases the error (deviance). As for the two different models with different penalty factors, it's visible from the second plot that $\lambda$ plays an important role on the flexibility of the model in general. A high lambda will penalize too hard the curve, while a low lambda will let the model follow more closely the trends and movement of the curve. 

Finally the relationship of the degrees of freedom with respect to $\lambda$ is given by the following formula.

$$df_{\lambda}=\sum_{k=1}^{N}\frac{1}{1+\lambda d_k}$$

It's visible from the formula that the greater the $\lambda$ the lower the degrees of freedom. This relationship can be seen on the last plot, where the effective degrees of freedom are shown against the penalty factor. We see a rapid decrease of the degrees which confirms this relationship.

```{r}
############
# TASK 1.4 #
############

# Creating grid for the penalty factor.
lambdas = seq(0.0 , 25, 0.1)

# Data frame that is going to hold
# the information of all the models.
results = NULL

for (lambda in lambdas)
{
  # Fitting a GAM for each penalty factor lambda.
  g_reg = gam(Mortality ~ Year + s(Week,
                                   k=length(unique(data$Week)),
                                   sp=lambda), 
              data=data)
  
  # Adding the information to the data frame.
  results = rbind(results, data.frame(list(lambda=lambda,
                                           deviance=g_reg$deviance,
                                           df=sum(influence(g_reg)))))
}

# Plotting the deviance vs lambda.
p = ggplot() + 
    geom_line(aes(x=results$lambda, y=results$deviance)) +
    labs(x='Lambda',
         y='Deviance',
         title='Deviance vs Lambda') 
  

print(p)

# High vs low penalty factors.
low_reg = gam(Mortality ~ Year + s(Week,
                                   k=length(unique(data$Week)),
                                   sp=0.01),
              data=data)

high_reg = gam(Mortality ~ Year + s(Week,
                                    k=length(unique(data$Week)),
                                    sp=10),
               data=data)

preds = NULL
preds = rbind(preds, data.frame(list(y=data$Mortality,
                                     x=data$Date,
                                     type='True values')))
preds = rbind(preds, data.frame(list(y=fitted(low_reg),
                                     x=data$Date,
                                     type='low (0.01)')))
preds = rbind(preds, data.frame(list(y=fitted(high_reg),
                                     x=data$Date,
                                     type='high (10.0)')))


# Plotting the predictions with low/high penalty
# and the real values.

p = ggplot() +
    geom_line(aes(x=preds$x, y=preds$y, colour=preds$type)) +
    scale_colour_manual(values=c("#9699bc", "#1162bc", "#f83d69")) +
    labs(x='Time',
         y='Mortality',
         colour='Type of prediction',
         title='Mortality vs Time')

print(p)

# Plotting the deviance vs lambda.
p = ggplot() + 
    geom_line(aes(x=results$lambda, y=results$df)) +
    labs(x='Lambda',
         y='Degrees of Freedom',
         title='Degrees of Freedom vs Lambda')

print(p)

```

## Task 1.5

**Task:** Use the model obtained in step $2$ and plot the residuals and the influenza values against time (in one plot). Is the temporal pattern in the residuals correlated to the outbreaks of influenza?

**Answer:** From the plot shown below it seems that there is some temporal correlation between the residuals and the outbreaks of influenza. This is expected since it was seen on task 1.1 that the mortality and outbreaks of influenza are closely related and that the variable `Influeza` was not added to our linear model. So, the residuals must show this pattern that is not explain solely by the years and week number.

```{r}
############
# TASK 1.5 #
############

# Getting the residuals.
resid = residuals(reg)

# Plotting the residuals and influenza vs the time.
p = ggplot() + 
    geom_line(aes(x=data$Date,y=resid, colour='Residuals')) +
    geom_line(aes(x=data$Date,y=data$Influenza, colour='Influenza')) +
    labs(x='Date',
         y='Influenza / Residuals',
         title='Influenza/Residuals vs time') +
    scale_colour_manual(values=c("#1162bc", "#f83d69"))

print(p)

```

## Task 1.6

**Task:** Fit a `GAM` model in R in which moratlity is be modelled as an additive function of the spline functions of year, week, and the number of confirmed cases of influenza. Use the output of this `GAM` function to conclude whether or not the mortality is influenced by the outbreaks of influenza. Provide the plot of the original and fitted Mortality against Time and comment whether the model seems to be better than the previous GAM models.

**Answer:** From the first plot we see that the model now present a better fit than the last model by incorporating the years and the influenza as a spline. From the summary we can also see that the year is not statistically significant which is congruent with the result shown previously. While the variables `Week` and `Influenza` are statistically significant. These two facts can be seen from the spline components. The components from `Year` are almost zero, while for `Week` and for `Influenza` are different and follows a pattern which is descriptive to the model. From the last two plots we can see that the residuals no longer follows the pattern of the influenza since this pattern is being captured by the new spline and that the residuals seems to behave as a normal distribution, which indicates that the model is a good fit. With all this information taken into account, is possible to conclude that `Influenza` seems to influence the `Mortality` in Sweden and that this model is a better fit compared to the one found in task 1.3 since it has a better $R^2$ ($0.819$ vs $0.677$).

```{r, message = FALSE}
############
# TASK 1.6 #
############

# Fitting the new model.
reg = gam(Mortality ~ s(Year, k=length(unique(data$Year))) +
                      s(Week, k=length(unique(data$Week))) +
                      s(Influenza, k=length(unique(data$Influenza))),
                      data=data)

# Getting the fitted values and
# creating a plot of 
# y vs yhat.
y_hat = fitted(reg)

# Plotting the real values and the fitted ones.
p = ggplot() +
    geom_line(aes(x=data$Date, y=data$Mortality, color='True values')) +
    geom_line(aes(x=data$Date, y=y_hat, color='Fitted values')) +
    scale_colour_manual(values=c("#604dc5","#f83d69")) +
    labs(x='Time',
         y='Mortality',
         title='True values and predicted values of Mortality')

print(p)

# Output of the regression object.
summary(reg)

# Plotting the spline component.
plot(reg)

# Getting the residuals.
resid = residuals(reg)

# Plotting the residuals and influenza vs the time.
p = ggplot() + 
    geom_line(aes(x=data$Date,y=resid, colour='Residuals')) +
    geom_line(aes(x=data$Date,y=data$Influenza, colour='Influenza')) +
    labs(x='Date',
         y='Influenza / Residuals',
         title='Influenza/Residuals vs time') +
    scale_colour_manual(values=c("#1162bc", "#f83d69"))

print(p)

# Plotting the histogram from the residuals.
p = ggplot() +
    geom_histogram(aes(resid))

print(p)

```

# Assignment 2

The data file **data.csv** contains information about 64 em-mails which were manually collected from DBWorld mailing list. They were classified as: 'announces of conferences' (1) and 'everything else' (0) (variable Conference)

## Task 2.1

**Task:** Divide data into training and test sets (70/30) without scaling. Perform nearest shrunken centroid classification of training data in which the threshold is chosen by cross-validation. Provide a centroid plot and interpret it. How many features were selected by the method? List the names of the 10 most contributing features and comment whether it is reasonable that they have strong effect on the discrimination between the conference mails and other mails? Report the test error.

**Answer:** We see that the optimal value for the threshold is around $1$ and $2$. In this case, the value 1.25 was selected. The number of features selected given this threshold is $231$. These features can be seen below on the centroid plot. This plot shows the shrunken differences $d_{ik}^{'}$ for feature $i$ and class $k$. This difference can be thought of as a distance measure of the intra-feature centroid per class ${\overline{x}}_{ik}$ versus the class centroid ${\overline{x}}_i$. So, this plot shows the features which have the biggest distance between the overall mean of the feature versus the intra-class mean of the feature. So, for example, we can see that `papers` is the variable that has the biggest difference between observations that came from a `Conference` email and observations who doesn't, since it has the largest shrunken difference per class. 

The top contributing features are presented on the table 1. It seems reasonable that these features will have a strong effect on the discrimination between the conference mails and other mails since it present terms related with `Conferences`, like `papers`, `published` and even `conference` itself. The classifier NSC have a really good performance in the sence that it has an FPR of $0$, a missclasification of $10\%$ and a precision of $100\%$.

```{r message=FALSE, warning=FALSE, results='hide'}
############
# TASK 2.1 #
############

# Loading the data and doing preprocessing.
data = read.csv('data.csv', sep=';', encoding='latin1')
data$Conference = as.factor(data$Conference)

# Traing / test split.
set.seed(12345)
n = dim(data)[1]
id = sample(1:n, floor(n * 0.7))
train = data[id,]
test = data[-id,]

rownames(train) = 1:nrow(train)
X_train = t(train[, -ncol(train)])
y_train = train$Conference

rownames(test) = 1:nrow(test)
X_test = t(test[, -ncol(test)])
y_test = test$Conference

# Doing weird stuff because that's what
# the library wants.
mydata_train = list(x=X_train,
                    y=y_train,
                    geneid=as.character(1:nrow(X_train)),
                    genenames=rownames(X_train))

mydata_test = list(x=X_test,
                    y=y_test,
                    geneid=as.character(1:nrow(X_test)),
                    genenames=rownames(X_test))

# Training the model and getting the best parameter by cv.
model = pamr.train(mydata_train, threshold=seq(0, 4, 0.1))
cv_model = pamr.cv(model, mydata_train)

th = cv_model$threshold[which.min(cv_model$error)]

```

Selected the threshold by:

```{r, echo = TRUE}

cv_model$threshold[which.min(cv_model$error)]

```


```{r fig.height=10, message=FALSE}
pamr.plotcv(cv_model)

```

```{r fig.height=10, message=FALSE}
pamr.plotcen(model, mydata_train, threshold=th)

```

```{r}
# Helper function to calculate the
# classification metrics given a
# confusion matrix.
# Rows (0, 1) must be from the classifier
# Columns (0, 1) must be true values.
analyze_cm = function(cm)
    {
        cm_df = as.data.frame(cm)
        recall = cm_df[4, "Freq"] / (cm_df[4, "Freq"] + cm_df[2, "Freq"])
        precision = cm_df[4, "Freq"] / (cm_df[4, "Freq"] + cm_df[3, "Freq"])
        accuracy = (cm_df[1, "Freq"] + cm_df[4, "Freq"]) / sum(cm_df[, "Freq"])
        mcr = 1 - accuracy
        fpr = cm_df[3, "Freq"] / (cm_df[1, "Freq"] + cm_df[3, "Freq"])
    
        return(list(Recall=recall,
                    Precision=precision,
                    Missclasification=mcr,
                    Accuracy=accuracy,
                    FPR=fpr))
}

# Helper function to get metrics from a confusion matrix.
get_metrics_cm = function(cm_train, cm_test)
{
  metrics = data.frame(analyze_cm(cm_train))
  metrics = rbind(metrics, analyze_cm(cm_test))
  rownames(metrics) = c("Train", "Test")
  
  return(t(metrics))
}

# Transforms a confusion matrix
# to a data frame. It assumes
# a 2 by 2 binary confusion matrix.
prettyfy_cm = function(cm_table)
{
  pretty_table = data.frame(list(titles=c("Not Conference", "Conference"),
                                 Bad=c(cm_table[1, 1], cm_table[2, 1]),
                                 Good=c(cm_table[1, 2], cm_table[2, 2])))
  
  colnames(pretty_table) = c("True / Predicted", "Not Conference", "Conference")
  
  return(pretty_table)
}
```

```{r warning=FALSE, message=FALSE}

genes = pamr.listgenes(model, mydata_train, threshold=th)
top_genes = as.data.frame(colnames(data)[as.numeric(genes[1:10, 1])])
colnames(top_genes) = 'Features'
kable(top_genes, caption='Top contributing features')

# Getting the prediction.
yhat_train = pamr.predict(model, newx=X_train, threshold=th)
yhat_test = pamr.predict(model, newx=X_test, threshold=th)

# Computing the confusion matrixes.
cm_train = table(train$Conference, yhat_train)
cm_test = table(test$Conference, yhat_test)

# Pretty printing.
kable(prettyfy_cm(cm_train), caption='Confusion matrix of the traint set')
kable(prettyfy_cm(cm_test), caption='Confusion matrix of the test set')

kable(get_metrics_cm(cm_train, cm_test), caption='Metrics of the train / test set')

```


## Task 2.2

**Task:** Compute the test error and the number of the contributing features for the following methods fitted to the train data:

a. Elastic net with the binomial response and $\alpha = 0.5$ in which penalty is selected by the cross-validation

b. Support vector machine with "vanilladot" kernel.

Compare the results of these models with the results of the nearest shrunken centroids (make a comparative table). Which model would you prefer and why?

**Answer:** In this case the best model given the test set is SVM because it outperforms every other model. However it uses all of the features while NSCM have a similar performance but only uses 231 features out of 4703. In this case, the more favorable model is SVM because so far the calculations are not so computational heavy, however, if the data set where to increase and it needs to be deployed on a computer with little resources, NSCM would be the best model overall.

```{r message=FALSE, warning=FALSE, results = 'hide'}
############
# TASK 2.2 #
############

# Creating the train/test data.
X_train = as.matrix(train[, -ncol(train)])
y_train = train$Conference

X_test = as.matrix(test[, -ncol(test)])
y_test = test$Conference

# Creating the elastic net and SVM.
elastic = cv.glmnet(x=X_train, y=y_train, alpha=0.5, family="binomial")
svm = ksvm(X_train, y_train, kernel='vanilladot', scale=FALSE)

# Getting the confusion matrixes.
cm_train_elastic = table(y_train,
                         predict(elastic,
                                 newx=X_train,
                                 type='class'))
cm_test_elastic = table(y_test,
                        predict(elastic,
                                newx=X_test,
                                type='class'))

cm_train_svm = table(y_train,
                     predict(svm,
                             X_train,
                             type='response'))
cm_test_svm = table(y_test,
                    predict(svm,
                            X_test,
                            type='response'))

# Getting the number of features used
# by each algorithm.
elastic_coef = sum(coef(elastic, s='lambda.min') != 0) - 1
svm_coef = dim(data)[2]

# Getting the metrics for each algorithm.
nscm_metrics = get_metrics_cm(cm_train, cm_test)
colnames(nscm_metrics) = c("NSCM Train", "NSCM Test")
nscm_metrics = rbind(nscm_metrics,
                     N_Features=c(dim(genes)[1],
                                  dim(genes)[1]))

elastic_metrics = get_metrics_cm(cm_train_elastic, cm_test_elastic)
colnames(elastic_metrics) = c("Elastic Net Train", "Elastic Net Test")
elastic_metrics = rbind(elastic_metrics,
                        N_Feature=c(elastic_coef,
                                    elastic_coef))

svm_metrics = get_metrics_cm(cm_train_svm, cm_test_svm)
colnames(svm_metrics) = c("SVM Train", "SVM Test")
svm_metrics = rbind(svm_metrics,
                    N_Feature=c(svm_coef,
                                svm_coef))

all_metrics = cbind(nscm_metrics, elastic_metrics, svm_metrics)


```

```{r}

kable(all_metrics, caption="Comparison between all models")

```


## Task 2.3

**Task:** Implement Benjamini-Hochberg method for the original data, and use t.test() for computing p-values. Which features correspond to the rejected hypotheses? Interpret the result.

**Answer:** With a predefined false discovery rate (FDR) of $5\%$ we end up selecting 39 features which are shown below with the respective $p-value$ plots and the BH rejection threshold. We basically are testing multiple hypothesis at the same time and we know the following bound exist over the real false discovery rate.

$$FDR \leq \alpha \frac{M_0}{M} \leq \alpha$$

With this bound in mind we create a set of significant features which is defined as follows:

$$S = \left\{j:p_{(j)} < \alpha\frac{j}{M}\right\}$$

Where $p_{(j)}$ is the p-value of the ordered feature $j$. So this set of features are those who, under the assumptions of the t-test, have a statistically different mean for the observations that belong to a `Conference` and those who doesn't. We see that the selected features are coherent with was told above. For example, the word `phd` or `workshop` are strongly related with a `Conference` and not so much with other type of emails, which means that one would expect a difference in mean under the two populations (one that came from conferences and the other who doesn't).

```{r}
############
# TASK 2.3 #
############

# Variable that is going to store
# all of the p-values of each feature.
p_values = c()

# Calculating all of the p-values given a t-test.
for (i in 1:(ncol(data) - 1))
{
  test = t.test(data[, i] ~ data$Conference)
  p_values = c(p_values, test$p.value)
}

# Ordering the p-values.
ordered_idxs = base::order(p_values)
ordered_pvalues = p_values[ordered_idxs]

# Defining the BJ rejection threshold.
bh_rejection = function(j, alpha=0.15, M=4702)
{
  return(alpha * j / M)
}

# Populating the threshold.
fpr = 0.05
threshold = sapply(1:4702, bh_rejection, fpr)

# Merging everything into a dataframe.
results = data.frame(list(index=1:(ncol(data) - 1),
                          ordered_pvalues=ordered_pvalues,
                          threshold=threshold,
                          significant=threshold > ordered_pvalues))

p = ggplot(results) +
    geom_point(aes(x=index, y=ordered_pvalues, colour=significant)) +
    geom_line(aes(x=index, y=threshold, colour='BH Rejection Threshold')) +
    scale_colour_manual(values=c("#000000", "#1162bc", "#f83d69")) +
    labs(x='Ordered p-values index', y='p-value', colour='Significant')

print(p)

p = ggplot(results) +
    geom_point(aes(x=index, y=log(ordered_pvalues), colour=significant)) +
    geom_line(aes(x=index, y=log(threshold), colour='Log(BH Rejection Threshold)')) +
    scale_colour_manual(values=c("#1162bc", "#000000", "#f83d69")) +
    labs(x='Ordered p-values index', y='Log(p-value)', colour='Significant')

print(p)

# Slicing the selected features.
Features = colnames(data)[ordered_idxs[1:39]]
Features = data.frame(Features)

# Printing the selected features.
kable(Features, 
      caption='Features selected by the Benjamini-Hochberg method')
```

# Appendix

```{r, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```