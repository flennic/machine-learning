---
title: "Lab 3 Block 1: Kernel Meethods and Neural Networks"
author: "Maximilian Pfundstein"
date: "9 December 2018"
output:
  html_document:
    df_print: paged
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: 3
  word_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
library(geosphere)
library(ggplot2)
library(kernlab)

```

# Kernel Methods

**Task:**

- Implement a kernel method to predict the hourly temperatures for a date and place in Sweden.
- You are asked to provide a temperature forecast for a date and place in Sweden. The
forecast should consist of the predicted te mperatures from 4 am to 24 pm in an interval of 2
hours.
- Use a kernel that is the sum of three Gaussian kernels.
  - The first to account for the distance from a station to the point of interest.
  - The second to account for the distance between the day a temperature measurement
was made and the day of interest.
  - The third to account for the distance between the hour of the day a temperature measurement
was made and the hour of interest.
- Choose an appropriate smoothing coefficient or width for each of the three kernels above.
- Show that your choice for the kernels' width is sensible, i.e. that it gives more weight
to closer points. Discuss why your of definition of closeness is reasonable.
- Instead of combining the three kernels into one by summing them up, multiply them.
Compare the results obtained in both cases and elaborate on why they may differ.

*Note that the file temps50k.csv may contain temperature measurements that are posterior
to the day and hour of your forecast. You must filter such measurements out, i.e. they cannot
be used to compute the forecast. Feel free to use the template below to solve the assignment.*

```{r, echo = FALSE}

################################################################################
# Kernel Methods
################################################################################

## Helper Functions

min_distance = function(a, b, ringSize) {
  
  boundOne = sapply(b, FUN = function(x) abs(a - x))
  boundTwo = sapply(b, FUN = function(x) abs(a+ringSize-x))
  
  return(pmin(boundOne, boundTwo))
}


## Kernels
## The definition for the Guassian Kernel is taken from the slides, page 6.

kernel_gauss_distance = function(pointA, pointB, smoothing) {
  # Use distHaversine() as a help
  # Needed as distHaversine does not operate on dataframes with more than two
  # columns
  # latitude
  # longitude
  m = cbind(pointB$longitude, pointB$latitude)
  u = distHaversine(m, c(pointA$longitude, pointA$latitude))
  u = u / smoothing
  return(exp(-(u^2)))
}

kernel_gauss_day = function(dayA, dayB, smoothing) {
  
  dayA_in_days = as.numeric(strftime(as.Date(dayA$date), '%j'))
  dayB_in_days = as.numeric(strftime(as.Date(dayB$date), '%j'))
  
  u = min_distance(dayA_in_days, dayB_in_days, 365)
  u = u / smoothing
  return(exp(-(u^2)))
}

kernel_gauss_hour = function (hourA, hourB, smoothing) {
  
  hourA_in_h = sapply(hourA$time, FUN = function(x)
    as.numeric(difftime(strptime(x, format = "%H:%M:%S"),
                        strptime("00:00:00", format = "%H:%M:%S"))))
  
  hourB_in_h = sapply(hourB$time, FUN = function(x)
    as.numeric(difftime(strptime(x, format = "%H:%M:%S"),
                        strptime("00:00:00", format = "%H:%M:%S"))))
  
  u = min_distance(hourA_in_h, hourB_in_h, 24)
  u = u / smoothing
  return(exp(-(u^2)))
}

plot_values_distance = list(latitude = seq(from = 49, to = 51, by = 0.01),
                            longitude = seq(from = 49, to = 51, by = 0.01))

plot_values_day =
  list(date = c("2013-01-01", "2013-01-16", "2013-02-01", "2013-02-16",
                "2013-03-01", "2013-03-16", "2013-04-01", "2013-04-16",
                "2013-05-01", "2013-05-16", "2013-06-01", "2013-06-16",
                "2013-07-01", "2013-07-16", "2013-08-01", "2013-08-16",
                "2013-09-01", "2013-09-16", "2013-10-01", "2013-10-16",
                "2013-11-01", "2013-11-16", "2013-12-01", "2013-12-16"))

plot_values_hour = list(time = c("04:00:00", "05:00:00", "06:00:00", "07:00:00", "08:00:00",
                    "09:00:00", "10:00:00", "11:00:00", "12:00:00", "13:00:00",
                    "14:00:00", "15:00:00", "16:00:00", "17:00:00", "18:00:00",
                    "19:00:00", "20:00:00", "21:00:00", "22:00:00", "24:00:00"))

compare_value_distance = list(latitude = 50, longitude = 50)
compare_value_day = list(date = "2018-07-01")
compare_value_hour = list(time = "12:30:00")

# Smoothing factors
h_distance = 5000  # 5000
h_date = 7 # 7
h_time = 0.5 # 0.5

distances = kernel_gauss_distance(compare_value_distance, plot_values_distance, h_distance)
days = kernel_gauss_day(compare_value_day, plot_values_day, h_date)
hours = kernel_gauss_hour(compare_value_hour, plot_values_hour, h_time)

# Need to be continuous for plotting
plot_values_day = as.numeric(as.Date(plot_values_day$date))
plot_values_hour = as.numeric(strptime(plot_values_hour$time, format = "%H:%M:%S"))

# Dataframes for plotting
plot_data_frame_distance = data.frame(plot_values_distance$latitude, distances)
plot_data_frame_day = data.frame(plot_values_day, days)
plot_data_frame_hour = data.frame(plot_values_hour, hours)

```

```{r, echo = FALSE}

ggplot(plot_data_frame_distance) +
  geom_line(aes(x = plot_values_distance$latitude, y = distances,
                colour = "Distance (smoothing = 5000)")) +
  #geom_point(aes(x = plot_values_distance$latitude, y = distances),
  #           colour = "orange") +
  labs(title = "Gaussian Kernel Cruve With Selected Smoothing Factor", y = "Kernel Output",
       x = "Distance", color = "Legend") +
  scale_color_manual(values = c("orange"))
  
ggplot(plot_data_frame_day) +
  geom_line(aes(x = plot_values_day, y = days, colour = "Day (smoothing = 7)")) +
  #geom_point(aes(x = plot_values_day, y = days), colour = "blue") #+
  labs(title = "Gaussian Kernel Cruve With Selected Smoothing Factor", y = "Kernel Output",
       x = "Day", color = "Legend") +
  scale_color_manual(values = c("blue"))
  
ggplot(plot_data_frame_hour) +
  geom_line(aes(x = plot_values_hour, y = hours,
                colour = "Hour (smoothing = 26)")) +
  #geom_point(aes(x = plot_values_hour, y = hours), colour = "green")
  #
  labs(title = "Gaussian Kernel Cruves With Selected Smoothing Factor", y = "Kernel Output",
       x = "Hour", color = "Legend") +
  scale_color_manual(values = c("violet"))


```



```{r, echo = FALSE, eval = TRUE}

kernel_sum = function(A, B, h_distance, h_date, h_time) {
  return(
    kernel_gauss_distance(A, B, h_distance) + 
    kernel_gauss_day(A, B, h_date) + 
    kernel_gauss_hour(A, B, h_time)
  )
}

kernel_product = function(A, B, h_distance, h_date, h_time) {
  return(
    kernel_gauss_distance(A, B, h_distance) * 
    kernel_gauss_day(A, B, h_date) * 
    kernel_gauss_hour(A, B, h_time)
  )
}

predict_weather = function(u_latitude, u_longtitude, u_date, u_type) {
  
  # Data
  stations = read.csv("stations.csv", encoding = "UTF-8")
  temps = read.csv("temps50k.csv", encoding = "UTF-8")
  st = merge(stations, temps, by="station_number")
  
  # Filter all date posterior to the given date (doesn't make sense to predict
  # something that we already now)
  st = st[as.Date(st$date) < as.Date(u_date),]
  
  # Given Data
  # Each user input should be a list of:
  # latitude, longitude, date, time
  # time is created with the loop at we predict for every time for the given day
  
  # Times to predict
  times = c("04:00:00", "06:00:00", "08:00:00", "10:00:00", "12:00:00",
           "14:00:00", "16:00:00", "18:00:00", "20:00:00", "22:00:00",
           "24:00:00")
  
  # Temperatures to predict
  temp = vector(length=length(times))
  
  # Prediction for each temp to predict
  for (i in 1:length(times)) {
    
    # User data point for this iteration
    user_data_point = list(
      latitude = u_latitude,
      longitude = u_longtitude,
      date = u_date,
      time = times[i])
    
    if (u_type == "sum") {
      # Parameters: A, B, h_distance, h_date, h_time
      kernel = kernel_sum(user_data_point, st, h_distance, h_date, h_time)
    }
    else if (u_type == "prod") {
      # Parameters: A, B, h_distance, h_date, h_time
      kernel =
        kernel_product(user_data_point, st, h_distance, h_date, h_time)
    }
    else {
      stop("Du Nasenbaer!")
    }
    
    # Now that we have the kernel value, we can calcuate the actual prediction
    # Formula taken from slide 8 from 
    # "Histogram, Moving Window, and Kernel Regression"
    y = kernel %*% st$air_temperature/sum(kernel)
    
    # Let's save this predicted temperature
    temp[i] = y
  }
  return(data.frame(times, temp))
}

set.seed(1234567890)


# Linkoeping
latitude = 58.410809
longitude = 15.621373
date <- "2000-05-08" # The date to predict (up to the students)

```


```{r, echo = FALSE}

# Students' code here
pred_temp_sum = predict_weather(latitude, longitude, date, "sum")
pred_temp_prod = predict_weather(latitude, longitude, date, "prod")

plot_data_frame =
  data.frame(pred_temp_sum$times, pred_temp_sum$temp, pred_temp_prod$temp)
colnames(plot_data_frame) = c("hour", "predWithSum", "predWithProd")

ggplot(plot_data_frame) +
  geom_line(aes(x = hour, y = predWithSum, group = 1,
                colour = "Kernel Regression (Sum)")) +
  geom_point(aes(x = hour, y = predWithSum), colour = "orange") +
  geom_line(aes(x = hour, y = predWithProd, group = 1,
                colour = "Kernel Regression (Prod)")) +
  geom_point(aes(x = hour, y = predWithProd), colour = "blue") +
  labs(title = "Temperature Prediction with Kernel Regression (Sum and Prod)",
       y = "Temperature", x = "Hour of the Day", color = "Legend") +
  scale_color_manual(values = c("blue", "orange"))

```



# Support Vector Machines

**Task:**

Use the function ksvm from the R package kernlab to learn a SVM for classifying the spam dataset that is included with the package. Consider the radial basis function kernel (also known as Gaussian) with a width of 0.05. For the C parameter, consider values 0.5, 1 and 5. This implies that you have to consider three models.
- Perform model selection, i.e. select the most promising of the three models (use any method of your choice except cross-validation or nested cross-validation).
- Estimate the generalization error of the SVM selected above (use any method of your choice except cross-validation or nested cross-validation).
- Produce the SVM that will be returned to the user, i.e. show the code.
- What is the purpose of the parameter C?

```{r, echo = FALSE}

################################################################################
# Support Vector Machines
################################################################################

data(spam)
n = dim(spam)[1]
id = sample(1:n, floor(n*0.70))
train_spam = spam[id,]
val_spam = spam[-id,]

# Our three models depending on
model_svm_05 =
  ksvm(type ~ ., val_spam, kernel = "rbfdot", lambda = 0.05, C = 0.5)
model_svm_1 =
  ksvm(type ~ ., val_spam, kernel = "rbfdot", lambda = 0.05, C = 1)
model_svm_5 =
  ksvm(type ~ ., val_spam, kernel = "rbfdot", lambda = 0.05, C = 5)

model_svm_05_prediction = predict(model_svm_05, newdata = val_spam)
model_svm_1_prediction = predict(model_svm_1, newdata = val_spam)
model_svm_5_prediction = predict(model_svm_5, newdata = val_spam)

cm_svm_05 = table(val_spam$type, model_svm_05_prediction)
cm_svm_1 = table(val_spam$type, model_svm_1_prediction)
cm_svm_5 = table(val_spam$type, model_svm_5_prediction)

error_svm_05 = 1 - sum(diag(cm_svm_05)/sum(cm_svm_05))
error_svm_1 = 1 - sum(diag(cm_svm_1)/sum(cm_svm_1))
error_svm_5 = 1 - sum(diag(cm_svm_5)/sum(cm_svm_5))

```

The confusion matrices look like this (C = 0.5, C = 1, C = 5):

```{r, echo = FALSE}

print(cm_svm_05)
print(cm_svm_1)
print(cm_svm_5)

```

The error rates are (```C = 0.5```, ```C = 1```, ```C = 5```):

```{r, echo = FALSE}

print(error_svm_05)
print(error_svm_1)
print(error_svm_5)

```


Therefor we select the model where ```C = 5```.

```{r, echo = FALSE}

print(model_svm_5)

```


C defines the cost of the constraint violation penalty. The default value is set
to 1 and it is an factor added to the error function. Thos means, for ```C = 0.5```
the errors a halved and for ```C = 5``` they are five times as large. Keep in
mind that we also have $\epsilon$ which defines the threshold, when an error is
applied. C must always be greater than 0, otherwise there would never be an error.
Normally C and $\epsilon$ are selected by Cross-Validation.


# Appendix: Source Code

```{r, ref.label=knitr::all_labels(), echo = TRUE, eval = FALSE}

```