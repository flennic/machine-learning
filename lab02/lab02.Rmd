---
title: "Machine Learning Lab 02"
author: "Maximilian Pfundstein (maxpf364)"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc_float: true
    number_sections: true
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(ggplot2)
library(readxl)
library(tree)
library(e1071)
library(boot)
library(fastICA)
```

# Assignment 2: Analysis of Credit Scoring

## Import ```creditscoring.xls```

Let's import the data and have a look at it.

```{r, echo = FALSE}

set.seed(12345)
creditscoring = read_excel("./creditscoring.xls")
creditscoring$good_bad = as.factor(creditscoring$good_bad)
kable(head(creditscoring[,(ncol(creditscoring)-10):ncol(creditscoring)]),
      caption = "creditscoring.xls")

```

```{r, echo = FALSE}

n=dim(creditscoring)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=creditscoring[id,]

id1=setdiff(1:n, id)
set.seed(12345)
id2=sample(id1, floor(n*0.25))

valid=creditscoring[id2,]
id3=setdiff(id1,id2)
test=creditscoring[id3,]

```

## Decision Tree Fitting

**Task:** Fit a decision tree to the training data by using the following measures of impurity:

a. Deviance
b. Gini index

```{r, echo = FALSE}

# Create the models
decisionTree_deviance = tree(good_bad ~ ., data = train, split = "deviance")
decisionTree_gini = tree(good_bad ~ ., data = train, split = "gini")

```

```{r, echo = FALSE}

# Prediction
prediction_deviance_train =
  predict(decisionTree_deviance, newdata = train, type = "class")
prediction_deviance_test =
  predict(decisionTree_deviance, newdata = test, type = "class")

predictiona_gini_train =
  predict(decisionTree_gini, newdata = train, type = "class")
prediction_gini_test =
  predict(decisionTree_gini, newdata = test, type = "class")

```

### Deviance

The model for the decision tree using deviance.

```{r, echo = FALSE}

summary(decisionTree_deviance)
#plot(decisionTree_deviance)

```

The confusion matrix looks as follows:

```{r, echo = FALSE}

confusion_matrix_deviance = table(prediction_deviance_test, test$good_bad)
kable(confusion_matrix_deviance)

```

Therefore the error rate is:

```{r, echo = FALSE}

error_rate_deviance =
  1 - sum(diag(confusion_matrix_deviance)/sum(confusion_matrix_deviance))
print(error_rate_deviance)

```

### Gini

The model for the decision tree using gini.

```{r, echo = FALSE}

summary(decisionTree_gini)
#plot(decisionTree_gini)

```

The confusion matrix looks as follows:

```{r, echo = FALSE}

confusion_matrix_gini = table(prediction_gini_test, test$good_bad)
kable(confusion_matrix_gini)

```

Therefore the error rate is:

```{r, echo = FALSE}

error_rate_gini =
  1 - sum(diag(confusion_matrix_gini)/sum(confusion_matrix_gini))
print(error_rate_gini)

```

### Conclusions 

**Question:** Report the misclassification rates for the training and test data. Choose the measure providing the better results for the following steps.

**Answer:** The misqualification rate for the decision tree with deviance is ```0.33``` compared to the decision tree with gini as the classifier which has a misqualification rate of ```0.3666667```. Therefore we will continue with using the decision tree that uses **deviance** as the classifier.

## Finding the Optimal Tree

**Task**:

1. Use training and validation sets to choose the optimal tree depth.
2. Present the graphs of the dependence of deviances for the training and the validation data on the number of leaves.
3. Report the optimal tree, report it's depth and the variables used by the tree.
4. Interpret the information provided by the tree structure.
5. Estimate the misclassification rate for the test data.

### Optimal Tree Depth

```{r, echo = FALSE}

# Taken from the slides
trainScore = rep(0, 9)
testScore = rep(0, 9)

for(i in 2:9) {
  prunedTree = prune.tree(decisionTree_deviance, best = i)
  pred = predict(prunedTree, newdata = valid, type = "tree")
  trainScore[i] = deviance(prunedTree)
  testScore[i] = deviance(pred)
}

```

The best tree is the tree with index ```5``` and a test score of ```350.952```.

```{r, echo = FALSE}

## Add one as the trim the first index
optimalTreeIdx = which.min(testScore[-1]) + 1
optimalTreeScore = min(testScore[-1])

print(optimalTreeIdx)
print(optimalTreeScore)

```

### Dependency of Deviances

The following plot shows the number of leaves vs the deviance. The orange line indicates the training and the blue line the test deviance.

```{r, echo = FALSE}

plot(2:9, trainScore[2:9], type = "b", col = "orange", ylim = c(300,500),
     main = "Tree Depth vs Training/Test Score", ylab = "Deviance",
     xlab = "Number of Leaves")
points(2:9, testScore[2:9], type = "b", col = "blue")
legend("topright", legend = c("Train (orange)", "Test (blue)"))

```

### Optimal Tree

The following plot shows the optimal tree and it's variables. It has a depth of ```4```.

```{r, echo = FALSE}

optimalTree = prune.tree(decisionTree_deviance, best = optimalTreeIdx)
plot(optimalTree)
text(optimalTree, pretty = 1)
title("Optimal Tree")

```



### Interpretating the Tree Structure
The tree splits the data based on if the duration is smaller than ```43.5```. This means this is the feature where the tree evaluated the most influence on the prediction. We can see that the right side only has one more variable which is savings. On the left side the tree splits further, starting with the history as the second most important feature after deciding the duration. As the tree has more leaves to teh left side we can see, that splitting this data further makes more than than it would've been on teh right sind.

### Estimate of the Missclassification Rate

```{r, echo = FALSE}

prediction_optimalTree_test =
  predict(optimalTree, newdata = test, type = "class")

confusion_matrix_optimalTree = table(prediction_optimalTree_test, test$good_bad)

error_optimalTree =
  1 - sum(diag(confusion_matrix_optimalTree)/sum(confusion_matrix_optimalTree))

```

```{r, echo = FALSE}

summary(optimalTree)
kable(confusion_matrix_gini)
print(error_optimalTree)

```

The last value shows the misclassification error on the test data set.

## Naive Bayes

**Task: **

- Use training data to perform classification using Naive Bayes.
- Report the confusion matrices and misclassification rates for the training and for the test data.
- Compare the results with those from step 3.

### Classification with Naive Bayes

Let's train the model and have a look at the summary.

```{r, echo = FALSE}

naiveBayesModel = naiveBayes(good_bad ~ ., data = train)
summary(naiveBayesModel)

```


### Naive Bayes Confusion Matrices and Misclassification Rates

```{r, echo = FALSE}

# Prediction
prediction_bayes_train =
  predict(naiveBayesModel, newdata = train, type = "class")
prediction_bayes_test =
  predict(naiveBayesModel, newdata = test, type = "class")

confusion_matrix_bayes_train = table(prediction_bayes_train, train$good_bad)
confusion_matrix_bayes_test = table(prediction_bayes_test, test$good_bad)

error_bayes_train = 1 - sum(diag(confusion_matrix_bayes_train)/
                              sum(confusion_matrix_bayes_train))
error_bayes_test =  1 - sum(diag(confusion_matrix_bayes_test)/
                              sum(confusion_matrix_bayes_test))

```

Data for Naive Bayes on train:

```{r, echo = FALSE}

kable(confusion_matrix_bayes_train)
print(error_bayes_train)

```

Data for Naive Bayes on test:

```{r, echo = FALSE}

kable(confusion_matrix_bayes_test)
print(error_bayes_test)

```

### Comparison with Step 3

We can see that the misqualification rate for the optimized decision tree with ```0.2633333``` is better than the Naive Bayes approach with a rate of ```0.29```. We have to keep in mind that we first had to find the best tree and thus spend more time optimizing the hyper parameters.

## TPR, FPR and ROC Curves

```{r, echo = FALSE}

# prediction optimal tree
prediction_optimalTree_test_p =
  predict(optimalTree, newdata = test, type = "vector")
# prediction naive bayes
prediction_bayes_test_p =
  predict(naiveBayesModel, newdata = test, type = "raw")

pi = seq(from = 0.00, to = 1.0, by = 0.05)
fprs_tree = c()
tprs_tree = c()
fprs_bayes = c()
tprs_bayes = c()

for (i in pi) {
  current_tree_pi_confusion =
    table(test$good_bad, factor(prediction_optimalTree_test_p[,2] > i,
                                lev=c(TRUE, FALSE)))
  current_bayes_pi_confusion =
    table(test$good_bad, factor(prediction_bayes_test_p[,2] > i,
                                lev=c(TRUE, FALSE)))
  
  # FPR = FP / N-
  # TPR = TP / N+
  fprs_tree =c(fprs_tree, current_tree_pi_confusion[1,1]/
                 sum(current_tree_pi_confusion[1,]))
  tprs_tree = c(tprs_tree, current_tree_pi_confusion[2,1]/
                  sum(current_tree_pi_confusion[2,]))
  
  fprs_bayes = c(fprs_bayes, current_bayes_pi_confusion[1,1]/
                   sum(current_bayes_pi_confusion[1,]))
  tprs_bayes = c(tprs_bayes, current_bayes_pi_confusion[2,1]/
                   sum(current_bayes_pi_confusion[2,]))
}

roc_values = data.frame(fprs_tree, tprs_tree, fprs_bayes, tprs_bayes)

```

**Task:** Compute the TPR and FPR values for the two models.

The corresponding values for FPR and TPR can be seen in the following table.

```{r, echo = FALSE}

kable(roc_values)

```

**Task:** Plot the corresponding ROC curves.

This is the ROC curve of the Optimized Tree and Naive Bayes.

```{r, echo = FALSE}

ggplot(roc_values) +
  geom_line(aes(x = fprs_tree, y = tprs_tree,
                colour = "ROC Optimized Tree")) +
  geom_point(aes(x = fprs_tree, y = tprs_tree), colour = "orange") +
  
  geom_line(aes(x = fprs_bayes, y = tprs_bayes,
                colour = "ROC Naive Bayes")) +
  geom_point(aes(x = fprs_bayes, y = tprs_bayes), colour = "blue") +
  
  labs(title = "ROC for Optimized Tree and Naive Bayes", y = "TPR",
       x = "FPR", color = "Legend") +
  scale_color_manual(values = c("blue", "orange"))

```

**Question:** Conclusion?

**Answer:** The ROC (receiver operating characteristic) curve shows how the models behaves for different threshold values. The greater the area under the curve the better the model can distinguish between two classes. This is due to the fact that we have overlapping distributions, which in worst case, are exactly on top of each other, which would result in a line at the 45 degree angle. As the distributations shift apart, our models will get better on average. Here we can observe, that the Naive Bayes is in general better in distinguishing the two classes, the Optimized Tree mostly performs worse. We have to keep in mind that we have few datapoints for small FPR for the tree and that we've not spent any effort interpolating the line between those two points.

## Naive Bayes Classification with Loss Matrix

**Task: **

- Repeat Naive Bayes classification as it was in step 4 but use the following loss
matrix.
- Report the confusion matrix for the training and test data.
- Compare the results with the results from step 4 and discuss how the rates has changed
and why.

This is the given loss matrix:

```{r, echo = FALSE}

L = matrix(c(0, 10, 1, 0), nrow = 2)
colnames(L) = c("Predicted", "Predicted")
rownames(L) = c("good", "bad")
kable(L)

```

```{r, echo = FALSE}

# Prediction
prediction_bayes_train_raw =
  predict(naiveBayesModel, newdata = train, type = "raw")
prediction_bayes_test_raw =
  predict(naiveBayesModel, newdata = test, type = "raw")

confusion_matrix_bayes_train =
  table(prediction_bayes_train_raw[,1]/
          prediction_bayes_train_raw[,2] > 10, train$good_bad)
confusion_matrix_bayes_test =
  table(prediction_bayes_test_raw[,1]/
          prediction_bayes_test_raw[,2] > 10, test$good_bad)

error_bayes_train_raw = 1 - sum(diag(confusion_matrix_bayes_train)/
                              sum(confusion_matrix_bayes_train))
error_bayes_test_raw =  1 - sum(diag(confusion_matrix_bayes_test)/
                              sum(confusion_matrix_bayes_test))
```


Confusion Matrix for Training:

```{r, echo = FALSE}

kable(confusion_matrix_bayes_train)
print(error_bayes_train_raw)

```

Confusion Matrix for Test:

```{r, echo = FALSE}

kable(confusion_matrix_bayes_test)
print(error_bayes_test_raw)

```

The error rates are way higher which was to be expected. There will always be the $\alpha$ and $\beta$ error. Choosing a loss matrix or setting a threshold for the classification will have influence on these errors. While making one of these errors small, the other one gets larger (with peaks where you classify everyhting as TRUE or FALSE). Here we defined that the confidence for being bad must be at least ten times larger than for being good which concludes in the results we've calculated and a high error rate. We have a really low false positive rate (with just 14 out of 286 being missclassified) in contrast to a high false negative rate (21 out of 35 being missclassified).

# Assignment 3: Uncertainty Estimation

## Import and Plot ```State.csv```

**Task: **

- Reorder your data with respect to the increase of MET and plot EX versus MET.
- Discuss what kind of model can be appropriate here.

Let's import the data and have a look at it:

```{r, echo = FALSE}

statedata = read.csv("./State.csv", sep = ";")
kable(head(statedata),  caption = "State.csv")

```

Let's plot the data:

```{r, echo = FALSE, warning = FALSE, message = FALSE}

statedata$MET = as.numeric(statedata$MET)
statedata = statedata[order(statedata$MET),] 
ggplot(statedata, aes(x = MET, y = EX)) +  geom_point() + geom_smooth()

```

At a first glance this data looks messy, but taking a second look gives the impression that the data has different kind of levels. Therefor a Regression Tree would probably be a good model.

## Regression Tree Model

**Task:**

- Report the selected tree.
- Plot the original and the fitted data and histogram of residuals.
- Comment on the distribution of the residuals and the quality of the fit.

Let's create a Regression Tree Model and use Cross Validation to see which sizes is the best.

```{r, echo = FALSE}

# Create the model
reg_tree = tree(EX ~ MET, data = statedata, control =
                  tree.control(nobs = nrow(statedata), minsize = 8))

# Use cross validation
cross_val_reg_tree = cv.tree(reg_tree)

# Plot the deviance of the sizes
plot(cross_val_reg_tree)

```

We see that 3 or 4 would work for the size of the tree. For the following we will declare ```best = 3```.
Let's prune our best tree and have a look at it.

```{r, echo = FALSE}

# Let's create the pruned the with best set to 3 and get it's prediction
pruned_tree = prune.tree(reg_tree, best = 3)
pruned_tree_prediction = predict(pruned_tree, newdata = statedata, type = "vector")

# We create a data.frame to save our values to make it easier to plot the data
pruned_tree_plot_dataframe =
  data.frame(statedata$MET, statedata$EX, pruned_tree_prediction,
             abs(pruned_tree_prediction-statedata$EX))
names(pruned_tree_plot_dataframe) = c("met", "orignal_ex", "predicted_ex", "residual")

# Lets first plot the pruned tree
plot(pruned_tree)
text(pruned_tree, pretty = 1)
title("Optimal Tree with best = 3")

# Lets create a plot with the real and predictes values and highlight the
# residuals
ggplot(pruned_tree_plot_dataframe) +
  geom_point(aes(x = pruned_tree_plot_dataframe$met,
                 y = pruned_tree_plot_dataframe$orignal_ex),
             color = "black") +
  geom_point(aes(x = pruned_tree_plot_dataframe$met,
                 y = pruned_tree_plot_dataframe$predicted_ex),
             color = "darkblue") +
  geom_segment(mapping=aes(x=pruned_tree_plot_dataframe$met,
                           y=pruned_tree_plot_dataframe$orignal_ex,
                           xend=pruned_tree_plot_dataframe$met,
                           yend=pruned_tree_plot_dataframe$predicted_ex),
               color = "red", linetype = "dotted") +
  labs(title = "Original Data, Fitted Data and Residuals", y = "EX",
       x = "MET", color = "Legend")

```

And here we have the histrogram of the residuals.

```{r, echo = FALSE}

hist(pruned_tree_plot_dataframe$residual,
     col="indianred1", main = "Histogram of the Residuals", xlab = "Residuals")

```


The histogramm looks like a Chi-Squared distribution with ```k ~ 3```.

## Confidence Bands (non-parametric)

**Task:**

- Compute and plot the 95% confidence bands for the regression tree model from step 2 by using a non-parametric bootstrap.
- Comment whether the band is smooth or bumpy and try to explain why.
- Consider the width of the confidence band and comment whether results of the regression model in
step 2 seem to be reliable.

```{r, echo = FALSE}

# We take the function given from the slides and adjust to the tree
# computing bootstrap samples
f_non_p_bootstrap = function(data, ind) {
  
  # First take the subsample
  data1 = data[ind,]
  
  # Now create a tree with the same hyperparameters from that subsample
  tree_model = tree(EX ~ MET, data = data1,
                    control = tree.control(nrow(data),minsize = 8))
  tree_model_pruned = prune.tree(tree_model, best = 3)
  
  # Use that model to predict on the real data
  prediction = predict(tree_model_pruned, newdata = data)
  return(prediction)
}

# Lets create the Bootstrap (again taken from slides)
res = boot(statedata, f_non_p_bootstrap, R = 1000)

# Confidence Bands using envelope
ci_non_p_bootstrap = envelope(res)
ci_non_p_bootstrap_df = as.data.frame(t(ci_non_p_bootstrap$point))
names(ci_non_p_bootstrap_df) = c("upper_bound", "lower_bound")
pruned_tree_plot_dataframe =
  data.frame(pruned_tree_plot_dataframe, ci_non_p_bootstrap_df)

# Plot the data
ggplot(pruned_tree_plot_dataframe) +
  geom_point(aes(x = pruned_tree_plot_dataframe$met,
                 y = pruned_tree_plot_dataframe$orignal_ex),
             color = "black") +
  geom_ribbon(aes(x = pruned_tree_plot_dataframe$met,
                  ymin = ci_non_p_bootstrap_df$lower_bound,
                  ymax = ci_non_p_bootstrap_df$upper_bound),
              alpha = 0.4, fill = "indianred1", color = "red") +
    labs(title = "Confidence Bands (non-parametric)", y = "EX",
       x = "MET", color = "Legend")

```

The prediction interval is neither really smooth or bumby, it's somwhere in between.
**TODO**.

## Confidence Bands (parametric)

**Task:** 

- Compute and plot the 95% confidence and prediction bands the regression tree
model from step 2 by using a parametric bootstrap.
- Consider the width of the confidence band and comment whether results of the regression model in step 2 seem to be reliable. Does it look like only 5% of data are outside the prediction band? Should it be?

```{r, echo = FALSE}

# Again we take the sample from the slides and adjust it to our needs
# 1) Compute value mle
# 2) Write function ran.gen that depends on data and mle and which generates
# new data
# 3) Write function statistic that depend on data which will be generated by
# ran.gen and should return the estimator

## 1)
mle = pruned_tree

## 2)
rng = function(data, mle) {
  data1 = data.frame(EX=data$EX, MET=data$MET)
  n = length(data$EX)
  #generate new Price
  # summary needed to access the residuals
  data1$EX = rnorm(n, predict(mle, newdata=data1), sd(summary(mle)$residuals))
  return(data1)
}

## 3) f_non_p_bootstrap+ distribution N
f_p_bootstrap = function(data) {
  
  # The index is not needed any more as we don't take a sub-sample
  
  # Now create a tree with the same hyperparameters from that subsample
  tree_model = tree(EX ~ MET, data = data,
                    control = tree.control(nrow(data), minsize = 8))
  tree_model_pruned = prune.tree(tree_model, best = 3)
  
  # Use that model to predict on the real data
  prediction = predict(tree_model_pruned, newdata = data)
  
  return(prediction)
}

# Bootstrap
res2 = boot(statedata,
            statistic = f_p_bootstrap, R=1000, mle=mle,
            ran.gen=rng, sim="parametric")

# Confidence Bands using envolope
ci_p_bootstrap = envelope(res2)
ci_p_bootstrap_df = as.data.frame(t(ci_p_bootstrap$point))
names(ci_p_bootstrap_df) = c("upper_bound", "lower_bound")
pruned_tree_plot_dataframe_p =
  data.frame(pruned_tree_plot_dataframe, ci_p_bootstrap_df)

# Plot the data
ggplot(pruned_tree_plot_dataframe_p) +
  geom_point(aes(x = pruned_tree_plot_dataframe_p$met,
                 y = pruned_tree_plot_dataframe_p$orignal_ex),
             color = "black") +
  geom_ribbon(aes(x = pruned_tree_plot_dataframe_p$met,
                  ymin = ci_p_bootstrap_df$lower_bound,
                  ymax = ci_p_bootstrap_df$upper_bound),
              alpha = 0.4, fill = "indianred1", color = "red") +
    labs(title = "Confidence Bands (parametric)", y = "EX",
       x = "MET", color = "Legend")

```

```{r, echo = FALSE}

# Prediction Bands

# from slides
f_p_bootstrap_pb = function(data) {
  
  # The index is not needed any more as we don't take a sub-sample
  
  # Now create a tree with the same hyperparameters from that subsample
  tree_model = tree(EX ~ MET, data = data,
                    control = tree.control(nrow(data), minsize = 8))
  tree_model_pruned = prune.tree(tree_model, best = 3)
  
  # Use that model to predict on the real data
  prediction = predict(tree_model_pruned, newdata = data)
  
  # Add the rnrom to the prediction
  prediction_normal = rnorm(nrow(data), prediction, sd(summary(mle)$residual))
  
  return(prediction_normal)
}

# Bootstrap
res3 = boot(statedata, statistic = f_p_bootstrap_pb,
            R=1000, mle=mle, ran.gen=rng, sim="parametric")

# Confidence Bands using envolope
pb_p_bootstrap = envelope(res2)
pb_p_bootstrap_df = as.data.frame(t(pb_p_bootstrap$point))
names(pb_p_bootstrap_df) = c("upper_bound", "lower_bound")
pruned_tree_plot_dataframe_p_pb =
  data.frame(pruned_tree_plot_dataframe, pb_p_bootstrap_df)

# Plot the data
ggplot(pruned_tree_plot_dataframe_p_pb) +
  geom_point(aes(x = pruned_tree_plot_dataframe_p_pb$met,
                 y = pruned_tree_plot_dataframe_p_pb$orignal_ex),
             color = "black") +
  geom_ribbon(aes(x = pruned_tree_plot_dataframe_p_pb$met,
                  ymin = pb_p_bootstrap_df$lower_bound,
                  ymax = pb_p_bootstrap_df$upper_bound), alpha = 0.4,
              fill = "indianred1", color = "red") +
    labs(title = "Prediction Bands (parametric)", y = "EX",
       x = "MET", color = "Legend")

```

**Comment: ** **TODO**

## Conclusions

**Task: ** Consider the histogram of residuals from step 2 and suggest what kind of
bootstrap is actually more appropriate here.

**Answer: ** **TODO**


# Assignemnt 4: Principal Components

## Principal Component Analysis

**Task: **

- Conduct a standard PCA by using the feature space and provide a plot
explaining how much variation is explained by each feature.
- Does the plot show how many PC should be extracted?
- Select the minimal number of components explaining at least 99% of the total variance.
- Provide also a plot of the scores in the coordinates (PC1, PC2).
- Are there unusual diesel fuels according to this plot?

Let's import the data and have a look at it.

```{r, echo = FALSE}

set.seed(12345)
nir_spectra = read.csv2("./NIRspectra.csv")
kable(head(statedata),  caption = "NIRspectra.csv")

```

```{r, echo = FALSE}

# Copy to not modify the original dataset
nir_spectra_copy = nir_spectra
nir_spectra_copy$Viscosity = c()

# PCA
res = prcomp(nir_spectra_copy)

# Eigenvalues
lambda = res$sdev^2

# Proportion of variation
kable(head(sprintf("%2.3f",lambda/sum(lambda)*100)), caption = "Variance for each Feature")

# Plot
screeplot(res, main = "Variances for each Feature")

```

As we can see from the plot and the table, the first feature is responsible for ```93.332%``` of the variance and the second feature for ```6.262%```. Together those two make of of ```99.594%``` of the variance.

Let's have a look at the plot for PC1 and PC2.

```{r, echo = FALSE}

ggplot(as.data.frame(res$x)) +
  geom_point(aes(x = res$x[,1],
                 y = res$x[,2]),
             color = "indianred1") +
    labs(title = "PV1 vs. PC2", y = "PC2",
       x = "PC1", color = "Legend")

```

**TODO**

## Trace Plots

Make trace plots of the loadings of the components selected in step 1. Is there
any principle component that is explained by mainly a few original features?

```{r, echo = FALSE}

U = res$rotation
plot(U[,1], main = "Traceplot, PC1")
plot(U[,2], main = "Traceplot, PC2")

```

## ICA

- Perform Independent Component Analysis with the number of components selected in step 1.
- Compute W' = K * W
- Present the columns of W' in form of the trace plots.
- Compare with the trace plots in step 2 and make conclusions. What kind of measure is represented by the matrix W'?
- Make a plot of the scores of the first two latent features and compare it with the score plot from step 1.

```{r, echo = FALSE}

set.seed(12345)

ica_model = fastICA(X = nir_spectra_copy, n.comp = 2, alg.typ = "parallel",
                    fun = "logcosh", alpha = 1, method = "R", row.norm = FALSE,
                    maxit = 200, tol = 0.0001, verbose = FALSE)

W_dash = ica_model$K %*% ica_model$W

plot(W_dash[,1], main= "Column 1")
plot(W_dash[,2], main= "Column 2")


ggplot(as.data.frame(W_dash)) +
  geom_point(aes(x = W_dash[,1],
                 y = W_dash[,2]),
             color = "indianred1") +
    labs(title = "Feature 1 vs Feature 2", y = "Feature 2",
       x = "Feature 1", color = "Legend")

```

**Conclusion: ** **TODO**


# Appendix: Source Code

```{r, ref.label=knitr::all_labels(), echo = TRUE, eval = FALSE}

```
