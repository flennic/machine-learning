---
title: "Machine Learning Lab 02"
author: "Maximilian Pfundstein (maxpf364)"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc_float: true
    number_sections: true
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(ggplot2)
library(readxl)
library(tree)
library(e1071)
```

# Assignment 2: Analysis of Credit Scoring

## Import ```creditscoring.xls```

Let's import the data and have a look at it.

```{r, echo = FALSE}

set.seed(12345)
creditscoring = read_excel("./creditscoring.xls")
creditscoring$good_bad = as.factor(creditscoring$good_bad)
kable(head(creditscoring[,(ncol(creditscoring)-10):ncol(creditscoring)]),
      caption = "creditscoring.xls")

```

```{r, echo = FALSE}

n=dim(creditscoring)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.4))
train=creditscoring[id,]

id1=setdiff(1:n, id)
set.seed(12345)
id2=sample(id1, floor(n*0.3))

valid=creditscoring[id2,]
id3=setdiff(id1,id2)
test=creditscoring[id3,]

```

## Decision Tree Fitting

**Task:** Fit a decision tree to the training data by using the following measures of impurity:

a. Deviance
b. Gini index

```{r, echo = FALSE}

# Create the models
decisionTree_deviance = tree(good_bad ~ ., data = train, split = "deviance")
decisionTree_gini = tree(good_bad ~ ., data = train, split = "gini")

```

```{r, echo = FALSE}

# Prediction
prediction_deviance_train =
  predict(decisionTree_deviance, newdata = train, type = "class")
prediction_deviance_test =
  predict(decisionTree_deviance, newdata = test, type = "class")

predictiona_gini_train =
  predict(decisionTree_gini, newdata = train, type = "class")
prediction_gini_test =
  predict(decisionTree_gini, newdata = test, type = "class")

```

### Deviance

The model for the decision tree using deviance.

```{r, echo = FALSE}

summary(decisionTree_deviance)
#plot(decisionTree_deviance)

```

The confusion matrix looks as follows:

```{r, echo = FALSE}

confusion_matrix_deviance = table(prediction_deviance_test, test$good_bad)
kable(confusion_matrix_deviance)

```

Therefore the error rate is:

```{r, echo = FALSE}

error_rate_deviance =
  1 - sum(diag(confusion_matrix_deviance)/sum(confusion_matrix_deviance))
print(error_rate_deviance)

```

### Gini

The model for the decision tree using gini.

```{r, echo = FALSE}

summary(decisionTree_gini)
#plot(decisionTree_gini)

```

The confusion matrix looks as follows:

```{r, echo = FALSE}

confusion_matrix_gini = table(prediction_gini_test, test$good_bad)
kable(confusion_matrix_gini)

```

Therefore the error rate is:

```{r, echo = FALSE}

error_rate_gini =
  1 - sum(diag(confusion_matrix_gini)/sum(confusion_matrix_gini))
print(error_rate_gini)

```

### Conclusions 

**Question:** Report the misclassification rates for the training and test data. Choose the measure providing the better results for the following steps.

**Answer:** The misqualification rate for the decision tree with deviance is ```0.33``` compared to the decision tree with gini as the classifier which has a misqualification rate of ```0.3666667```. Therefore we will continue with using the decision tree that uses **deviance** as the classifier.

## Finding the Optimal Tree

**Task**:

1. Use training and validation sets to choose the optimal tree depth.
2. Present the graphs of the dependence of deviances for the training and the validation data on the number of leaves.
3. Report the optimal tree, report it's depth and the variables used by the tree.
4. Interpret the information provided by the tree structure.
5. Estimate the misclassification rate for the test data.

### Optimal Tree Depth

```{r, echo = FALSE}

# Taken from the slides
trainScore = rep(0, 9)
testScore = rep(0, 9)

for(i in 2:9) {
  prunedTree = prune.tree(decisionTree_deviance, best = i)
  pred = predict(prunedTree, newdata = valid, type = "tree")
  trainScore[i] = deviance(prunedTree)
  testScore[i] = deviance(pred)
}

```

The best tree is the tree with index ```5``` and a test score of ```350.952```.

```{r, echo = FALSE}

## Add one as the trim the first index
optimalTreeIdx = which.min(testScore[-1]) + 1
optimalTreeScore = min(testScore[-1])

print(optimalTreeIdx)
print(optimalTreeScore)

```

### Dependency of Deviances

The following plots shows the Tree Depth vs the Training Score. The orange line indicates the training and the blue line the test score.

```{r, echo = FALSE}

plot(2:9, trainScore[2:9], type = "b", col = "orange", ylim = c(325,475),
     main = "Tree Depth vs Training/Test Score", ylab = "Deviance",
     xlab = "Number of Leaves")
points(2:9, testScore[2:9], type = "b", col = "blue")
legend("topright", legend = c("Train (orange)", "Test (blue)"))

```

### Optimal Tree

The following plot shows the optimal tree and it's variables. It has a depth of ```4```.

```{r, echo = FALSE}

optimalTree = prune.tree(decisionTree_deviance, best = optimalTreeIdx)
plot(optimalTree)
text(optimalTree, pretty = 1)
title("Optimal Tree")

```



### Interpretating the Tree Structure
The tree splits the data based on if the duration is smaller than ```43.5```. This means this is the feature where the tree evaluated the most influence on the prediction. We can see that the right side only has one more variable which is savings. On the left side the tree splits further, starting with the history as the second most important feature after deciding the duration. As the tree has more leaves to teh left side we can see, that splitting this data further makes more than than it would've been on teh right sind.

### Estimate of the Missclassification Rate

```{r, echo = FALSE}

prediction_optimalTree_test =
  predict(optimalTree, newdata = test, type = "class")

confusion_matrix_optimalTree = table(prediction_optimalTree_test, test$good_bad)

error_optimalTree =
  1 - sum(diag(confusion_matrix_optimalTree)/sum(confusion_matrix_optimalTree))

```

```{r, echo = FALSE}

summary(optimalTree)
kable(confusion_matrix_gini)
print(error_optimalTree)

```

The last value shows the misclassification error on the test data set.

## Na?ve Bayes

**Task: **

- Use training data to perform classification using Na?ve Bayes.
- Report the confusion matrices and misclassification rates for the training and for the test data.
- Compare the results with those from step 3.

### Classification with Na?ve Bayes

Let's train the model and have a look at the summary.

```{r, echo = FALSE}

naiveBayesModel = naiveBayes(good_bad ~ ., data = train)
summary(naiveBayesModel)

```


### Na?ve Bayes Confusion Matrices and Misclassification Rates

```{r, echo = FALSE}

# Prediction
prediction_bayes_train =
  predict(naiveBayesModel, newdata = train, type = "class")
prediction_bayes_test =
  predict(naiveBayesModel, newdata = test, type = "class")

confusion_matrix_bayes_train = table(prediction_bayes_train, train$good_bad)
confusion_matrix_bayes_test = table(prediction_bayes_test, test$good_bad)

error_bayes_train = 1 - sum(diag(confusion_matrix_bayes_train)/
                              sum(confusion_matrix_bayes_train))
error_bayes_test =  1 - sum(diag(confusion_matrix_bayes_test)/
                              sum(confusion_matrix_bayes_test))

```

Data for Na?ve Bayes on train:

```{r, echo = FALSE}

kable(confusion_matrix_bayes_train)
print(error_bayes_train)

```

Data for Na?ve Bayes on test:

```{r, echo = FALSE}

kable(confusion_matrix_bayes_test)
print(error_bayes_test)

```

### Comparison with Step 3

We can see that the misqualification rate for the optimized decision tree with ```0.2633333``` is better than the Na?ve Bayes approach with a rate of ```0.29```. We have to keep in mind that we first had to find the best tree and thus spend more time optimizing the hyper parameters.

## TPR, FPR and ROC Curves

```{r, echo = FALSE}

# prediction optimal tree
prediction_optimalTree_test_p =
  predict(optimalTree, newdata = test, type = "vector")
# prediction naive bayes
prediction_bayes_test_p =
  predict(naiveBayesModel, newdata = test, type = "raw")

pi = seq(from = 0.00, to = 1.0, by = 0.05)
fprs_tree = c()
tprs_tree = c()
fprs_bayes = c()
tprs_bayes = c()

for (i in pi) {
  current_tree_pi_confusion =
    table(test$good_bad, factor(prediction_optimalTree_test_p[,2] > i,
                                lev=c(TRUE, FALSE)))
  current_bayes_pi_confusion =
    table(test$good_bad, factor(prediction_bayes_test_p[,2] > i,
                                lev=c(TRUE, FALSE)))
  
  # FPR = FP / N-
  # TPR = TP / N+
  fprs_tree =c(fprs_tree, current_tree_pi_confusion[1,1]/
                 sum(current_tree_pi_confusion[1,]))
  tprs_tree = c(tprs_tree, current_tree_pi_confusion[2,1]/
                  sum(current_tree_pi_confusion[2,]))
  
  fprs_bayes = c(fprs_bayes, current_bayes_pi_confusion[1,1]/
                   sum(current_bayes_pi_confusion[1,]))
  tprs_bayes = c(tprs_bayes, current_bayes_pi_confusion[2,1]/
                   sum(current_bayes_pi_confusion[2,]))
}

roc_values = data.frame(fprs_tree, tprs_tree, fprs_bayes, tprs_bayes)

```

**Task:** Compute the TPR and FPR values for the two models.

The corresponding values for FPR and TPR can be seen in the following table.

```{r, echo = FALSE}

kable(roc_values)

```

**Task:** Plot the corresponding ROC curves.

This is the ROC curve of the Optimized Tree and Na?ve Bayes.

```{r, echo = FALSE}

ggplot(roc_values) +
  geom_line(aes(x = fprs_tree, y = tprs_tree,
                colour = "ROC Optimized Tree")) +
  geom_point(aes(x = fprs_tree, y = tprs_tree), colour = "orange") +
  
  geom_line(aes(x = fprs_bayes, y = tprs_bayes,
                colour = "ROC Na?ve Bayes")) +
  geom_point(aes(x = fprs_bayes, y = tprs_bayes), colour = "blue") +
  
  labs(title = "ROC for Optimized Tree and Na?ve Bayes", y = "TPR",
       x = "FPR", color = "Legend") +
  scale_color_manual(values = c("blue", "orange"))

```

**Question:** Conclusion?

**Answer:** The ROC (receiver operating characteristic) curve shows how the models behaves for different threshold values. The greater the area under the curve the better the model can distinguish between two classes. This is due to the fact that we have overlapping distributions, which in worst case, are exactly on top of each other, which would result in a line at the 45 degree angle. As the distributations shift apart, our models will get better on average. Here we can observe, that the Na?ve Bayes is in general better in distinguishing the two classes, the Optimized Tree mostly performs worse. We have to keep in mind that we have few datapoints for small FPR for the tree and that we've not spent any effort interpolating the line between those two points.

## Na?ve Bayes Classification with Loss Matrix

**Task: **

- Repeat Na?ve Bayes classification as it was in step 4 but use the following loss
matrix.
- Report the confusion matrix for the training and test data.
- Compare the results with the results from step 4 and discuss how the rates has changed
and why.

This is the given loss matrix:

```{r, echo = FALSE}

L = matrix(c(0, 10, 1, 0), nrow = 2)
colnames(L) = c("Predicted", "Predicted")
rownames(L) = c("good", "bad")
kable(L)

```

```{r, echo = FALSE}

# Prediction
prediction_bayes_train_raw =
  predict(naiveBayesModel, newdata = train, type = "raw")
prediction_bayes_test_raw =
  predict(naiveBayesModel, newdata = test, type = "raw")

confusion_matrix_bayes_train = table(prediction_bayes_train_raw[,1]/prediction_bayes_train_raw[,2] > 10, train$good_bad)
confusion_matrix_bayes_test = table(prediction_bayes_test_raw[,1]/prediction_bayes_test_raw[,2] > 10, test$good_bad)

error_bayes_train_raw = 1 - sum(diag(confusion_matrix_bayes_train)/
                              sum(confusion_matrix_bayes_train))
error_bayes_test_raw =  1 - sum(diag(confusion_matrix_bayes_test)/
                              sum(confusion_matrix_bayes_test))
```


Confusion Matrix for Training:

```{r, echo = FALSE}

kable(confusion_matrix_bayes_train)
print(error_bayes_train_raw)

```

Confusion Matrix for Test:

```{r, echo = FALSE}

kable(confusion_matrix_bayes_test)
print(error_bayes_test_raw)

```

The error rates are way higher which was to be expected. There will always be the $\alpha$ and $\beta$ error. Choosing a loss matrix or setting a threshold for the classification will have influence on these errors. While making one of these errors small, the other one gets larger (with peaks where you classify everyhting as TRUE or FALSE). Here we defined that the confidence for being bad must be at least ten times larger than for being good which concludes in the results we've calculated and a high error rate. We have a really low false positive rate (with just 14 out of 286 being missclassified) in contrast to a high false negative rate (21 out of 35 being missclassified).

# Assignment 3: Uncertainty Estimation

## Blabla
```{r}

set.seed(12345)

```

```{r, echo = FALSE}
statedata = read.csv("./State.csv", sep = ";")
#creditscoring$good_bad = as.factor(creditscoring$good_bad)
kable(head(statedata),
      caption = "creditscoring.xls")

```

```{r, echo = FALSE}

statedata$MET = as.numeric(statedata$MET) # Because R is shit.
statedata = statedata[order(statedata$MET),] 
ggplot(statedata, aes(x = MET, y = EX)) +
  geom_point() + geom_smooth()

```
## Blabla


```{r}
c_tree = tree(EX ~ MET, data = statedata, control = tree.control(nobs = nrow(statedata), minsize = 8))
cross_val_tree = cv.tree(c_tree)

plot(cross_val_tree)

c_pruned_tree = prune.tree(c_tree, best = 3)

plot(c_pruned_tree)
text(c_pruned_tree, pretty = 1)
title("Optimal Tree")

pruned_tree_prediction = predict(c_pruned_tree, newdata = statedata, type = "vector")

pruned_tree_plot_dataframe = data.frame(statedata$MET, statedata$EX, pruned_tree_prediction,  abs(pruned_tree_prediction-statedata$EX))

names(pruned_tree_plot_dataframe) = c("met", "orignal_ex", "predicted_ex", "residual")

ggplot(pruned_tree_plot_dataframe) +
  geom_point(aes(x = pruned_tree_plot_dataframe$met, y = pruned_tree_plot_dataframe$orignal_ex), color = "black") +
  geom_point(aes(x = pruned_tree_plot_dataframe$met, y = pruned_tree_plot_dataframe$predicted_ex), color = "darkblue") +
  geom_segment(mapping=aes(x=pruned_tree_plot_dataframe$met, y=pruned_tree_plot_dataframe$orignal_ex, xend=pruned_tree_plot_dataframe$met, yend=pruned_tree_plot_dataframe$predicted_ex), color = "red", linetype = "dotted")


```

```{r}
hist(pruned_tree_plot_dataframe$residual, col="indianred1")
```

Chi Squared

# Assignemnt 4: Principal Components

```{r}

set.seed(12345)

```

# Appendix: Source Code

```{r, ref.label=knitr::all_labels(), echo = TRUE, eval = FALSE}

```
