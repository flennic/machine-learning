---
title: "Machine Learning Lab 02"
author: "Maximilian Pfundstein (maxpf364)"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
  html_document:
    df_print: paged
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(ggplot2)
library(readxl)
library(tree)
```

# Assignment 2: Analysis of Credit Scoring

## Import ```creditscoring.xls```

Let's import the data and have a look at it.

```{r, echo = FALSE}

set.seed(12345)
creditscoring = read_excel("./creditscoring.xls")
creditscoring$good_bad = as.factor(creditscoring$good_bad)
kable(head(creditscoring[,(ncol(creditscoring)-10):ncol(creditscoring)]), caption = "creditscoring.xls")

```

```{r, echo = FALSE}

n=dim(creditscoring)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.4))
train=creditscoring[id,]

id1=setdiff(1:n, id)
set.seed(12345)
id2=sample(id1, floor(n*0.3))

valid=creditscoring[id2,]
id3=setdiff(id1,id2)
test=creditscoring[id3,]

```

## Decision Tree Fitting

**Task:** Fit a decision tree to the training data by using the following measures of impurity:

a. Deviance
b. Gini index

```{r, echo = FALSE}

# Create the models
decisionTree_deviance = tree(good_bad ~ ., data = train, split = "deviance")
decisionTree_gini = tree(good_bad ~ ., data = train, split = "gini")

```

```{r, echo = FALSE}

# Prediction
prediction_deviance_train =
  predict(decisionTree_deviance, newdata = train, type = "class")
prediction_deviance_test =
  predict(decisionTree_deviance, newdata = test, type = "class")

predictiona_gini_train =
  predict(decisionTree_gini, newdata = train, type = "class")
prediction_gini_test =
  predict(decisionTree_gini, newdata = test, type = "class")

```

### Deviance

The model for the decision tree using deviance.

```{r, echo = FALSE}

summary(decisionTree_deviance)
#plot(decisionTree_deviance)

```

The confusion matrix looks as follows:

```{r, echo = FALSE}

confusion_matrix_deviance = table(prediction_deviance_test, test$good_bad)
kable(confusion_matrix_deviance)

```

Therefore the error rate is:

```{r, echo = FALSE}

error_rate_deviance =
  1 - sum(diag(confusion_matrix_deviance)/sum(confusion_matrix_deviance))
print(error_rate_deviance)

```

### Gini

The model for the decision tree using gini

```{r, echo = FALSE}

summary(decisionTree_gini)
#plot(decisionTree_gini)

```

The confusion matrix looks as follows:

```{r, echo = FALSE}

confusion_matrix_gini = table(prediction_gini_test, test$good_bad)
kable(confusion_matrix_gini)

```

Therefore the error rate is:

```{r, echo = FALSE}

error_rate_gini =
  1 - sum(diag(confusion_matrix_gini)/sum(confusion_matrix_gini))
print(error_rate_gini)

```

### Conclusions 

**Question:** Report the misclassification rates for the training and test data. Choose the measure providing the better results for the following steps.

**Answer:** The misqualification rate for the decision tree with deviance is ```0.33``` compared to the decision tree with gini as the classifier which has a misqualification rate of ```0.3666667```. Therefor we will continue with using the decision tree that uses **deviance** as the classifier.

## Finding the Optimal Tree

**Task**:

1. Use training and validation sets to choose the optimal tree depth.
2. Present the graphs of the dependence of deviances for the training and the validation data on the number of leaves.
3. Report the optimal tree, report it's depth and the variables used by the tree.
4. Interpret the information provided by the tree structure.
5. Estimate the misclassification rate for the test data.

### Optimal Tree Depth

```{r, echo = FALSE}

# Taken from the slides
trainScore = rep(0, 9)
testScore = rep(0, 9)

for(i in 2:9) {
  prunedTree = prune.tree(decisionTree_deviance, best=i)
  pred = predict(prunedTree, newdata = valid, type = "tree")
  trainScore[i] = deviance(prunedTree)
  testScore[i] = deviance(pred)
}

```

The best tree is the tree with index ```5``` and a test score of ```350.952```.

```{r, echo = FALSE}

## Add one as the trim the first index
optimalTreeIdx = which.min(testScore[-1]) + 1
optimalTreeScore = min(testScore[-1])

print(optimalTreeIdx)
print(optimalTreeScore)

```

### Dependency of Deviances

The following plots shows the Tree Depth vs the Training Score. The orange line indicates the training and the blue line the test score.

```{r, echo = FALSE}

plot(2:9, trainScore[2:9], type = "b", col = "orange", ylim = c(325,475),
     main = "Tree Depth vs Training/Test Score", ylab = "Deviance",
     xlab = "Number of Leaves")
points(2:9, testScore[2:9], type = "b", col = "blue")
legend("topright", legend = c("Train (orange)", "Test (blue)"))

```

### Optimal Tree

The following plot shows the optimal tree and it's variables. It has a depth of ```4```.

```{r, echo = FALSE}

optimalTree = prune.tree(decisionTree_deviance, best = optimalTreeIdx)
plot(optimalTree)
text(optimalTree, pretty = 1)
title("Optimal Tree")

```





### Iterpretating the Tree Structure
Some blabla must be added.

### Estimate of the Missclassification Rate

```{r, echo = FALSE}

prediction_optimalTree_test =
  predict(optimalTree, newdata = test, type = "class")

confusion_matrix_optimalTree = table(prediction_optimalTree_test, test$good_bad)

error_optimalTree =
  1 - sum(diag(confusion_matrix_optimalTree)/sum(confusion_matrix_optimalTree))

```

```{r, echo = FALSE}

summary(optimalTree)
kable(confusion_matrix_gini)
print(error_optimalTree)

```

## Naïve Bayes

# Assignment 3: Uncertainty Estimation

```{r}

set.seed(12345)

```

# Assignemnt 4: Principal Components

```{r}

set.seed(12345)

```

# Appendix: Source Code

```{r, ref.label=knitr::all_labels(), echo = TRUE, eval = FALSE}

```
