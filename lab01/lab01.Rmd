---
title: "Machine Learning Lab 01"
author: "Maximilian Pfundstein"
date: "11/15/2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(readxl)
library(MASS)
library(glmnet)
library(kknn)
library(knitr)
set.seed(12345)
```

## Assignment 1 - Spam Classification with Nearest Neighbors

### 1.1)
Let's import the data and have a look at it.

```{r, echo=FALSE}
#### Assigment 1.1 ####
spambase = read_excel("./spambase.xlsx")
kable(head(spambase[,1:10]))
```

We split the data in 50% training and 50% testing.

```{r, echo = FALSE}
set.seed(12345)
n = dim(spambase)[1]
id = sample(1:n, floor(n*0.5))
train = spambase[id,]
test = spambase[-id,]
```

### 1.2)

```{r, echo = FALSE, warning = FALSE}
#### Assigment 1.3 ####
spambase_model = glm(Spam ~ ., data = train, family = "binomial")

## Train
spambase_predict_train =
  predict(object = spambase_model, newdata = train, type = "response")
spambase_predict_train =
  apply(as.matrix(spambase_predict_train), c(1),
        FUN = function(x) return(x > 0.5))
confusion_matrix_train =
  as.matrix(table(spambase_predict_train, train$Spam))
colnames(confusion_matrix_train) =
  c("Spam", "Normal Mail")
rownames(confusion_matrix_train) =
  c("Classified as Spam","Classified as no-Spam")

## Test
spambase_predict_test =
  predict(object = spambase_model, newdata = test, type = "response")
spambase_predict_test =
  apply(as.matrix(spambase_predict_test), c(1),
        FUN = function(x) return(x > 0.5))
confusion_matrix_test =
  as.matrix(table(spambase_predict_test, test$Spam))
colnames(confusion_matrix_test) =
  c("Spam", "Normal Mail")
rownames(confusion_matrix_test) =
  c("Classified as Spam","Classified as no-Spam")
```

Let's see what the confusion matrices look like for the Logistic Regression with 0.5 threshold. First one is for the training, second one is for the test data.

```{r, echo = FALSE}
kable(confusion_matrix_train, caption = "Confusion Matrix (Training Data)")
kable(confusion_matrix_test,caption = "Confusion Matrix (Test Data)")
```

Misclassification rate for the training data:

```{r, echo = FALSE}
spam_not_detected_train =
  confusion_matrix_train[2,1]/sum(confusion_matrix_train[,1])
mail_missclassified_train =
  confusion_matrix_train[1,2]/sum(confusion_matrix_train[,2])

print(paste(sep = "", "Spam not detected: ",
            spam_not_detected_train))
print(paste(sep = "", "Mail missclassified as spam: ",
            mail_missclassified_train))
print(paste(sep ="", "Misclassification rate: ",
            (confusion_matrix_train[1,2]+confusion_matrix_train[2,1])/
              sum(confusion_matrix_train)))
```

And now for the test data:

```{r, echo = FALSE}
spam_not_detected_test =
  confusion_matrix_test[2,1]/sum(confusion_matrix_test[,1])
mail_missclassified_test =
  confusion_matrix_test[1,2]/sum(confusion_matrix_test[,2])
print(paste(sep = "", "Spam not detected: ",
            spam_not_detected_test))
print(paste(sep = "", "Mail missclassified as spam: ",
            mail_missclassified_test))
print(paste(sep ="", "Misclassification rate: ",
            (confusion_matrix_test[1,2]+confusion_matrix_test[2,1])/
              sum(confusion_matrix_test)))
```

We can observe that the model is of course working better for the training data. We have a high error rate ($\alpha$ and $\beta$) errors which means the model is not working that well.

### 1.3)

```{r, echo = FALSE, warning = FALSE}
#### Assigment 1.3 ####
spambase_model_two = glm(Spam ~ ., data = train, family = "binomial")

## train_two
spambase_predict_train_two =
  predict(object = spambase_model_two, newdata = train, type = "response")
spambase_predict_train_two =
  apply(as.matrix(spambase_predict_train_two), c(1),
        FUN = function(x) return(x > 0.9))
confusion_matrix_train_two =
  as.matrix(table(spambase_predict_train_two, train$Spam))
colnames(confusion_matrix_train_two) =
  c("Spam", "Normal Mail")
rownames(confusion_matrix_train_two) =
  c("Classified as Spam","Classified as no-Spam")

## test_two
spambase_predict_test_two =
  predict(object = spambase_model_two, newdata = test, type = "response")
spambase_predict_test_two =
  apply(as.matrix(spambase_predict_test_two), c(1),
        FUN = function(x) return(x > 0.9))
confusion_matrix_test_two =
  as.matrix(table(spambase_predict_test_two, test$Spam))
colnames(confusion_matrix_test_two) =
  c("Spam", "Normal Mail")
rownames(confusion_matrix_test_two) =
  c("Classified as Spam","Classified as no-Spam")
```

Let's see what the confusion matrices look like for the Logistic Regression with 0.9 threshold. First one is for the training, second one is for the test data.

```{r, echo = FALSE}
kable(confusion_matrix_train_two, caption = "Confusion Matrix (Training Data)")
kable(confusion_matrix_test_two, caption = "Confusion Matrix (Test Data)")
```

Misclassification rate for the training data:

```{r, echo = FALSE}
spam_not_detected_train_two =
  confusion_matrix_train_two[2,1]/sum(confusion_matrix_train_two[,1])
mail_missclassified_train_two =
  confusion_matrix_train_two[1,2]/sum(confusion_matrix_train_two[,2])
print(paste(sep = "", "Spam not detected: ", spam_not_detected_train_two))
print(paste(sep = "", "Mail missclassified as spam: ",
            mail_missclassified_train_two))
print(paste(sep ="", "Misclassification rate: ",
            (confusion_matrix_train_two[1,2]+confusion_matrix_train_two[2,1])/
              sum(confusion_matrix_train_two)))
```

And now for the test data:

```{r, echo = FALSE}
spam_not_detected_test_two =
  confusion_matrix_test_two[2,1]/sum(confusion_matrix_test_two[,1])
mail_missclassified_test_two =
  confusion_matrix_test_two[1,2]/sum(confusion_matrix_test_two[,2])
print(paste(sep = "", "Spam not detected: ",
            spam_not_detected_test_two))
print(paste(sep = "", "Mail missclassified as spam: ",
            mail_missclassified_test_two))
print(paste(sep ="", "Misclassification rate: ",
            (confusion_matrix_test_two[1,2]+confusion_matrix_test_two[2,1])/
              sum(confusion_matrix_test_two)))
```

We can see, that the overall misclassification rate increases drastically and we shifted the $\alpha$ and $\beta$ errors. This means we have very few spam that is not detected, but this is due to the fact that almost all mail is classified as spam and therefore we have a really high error rate on mails that are classified as spam.

### 1.4)

Train the model with K = 30 and apply it on the training and test data using kknn.

```{r, echo = FALSE}
#### Assigment 1.4 ####
kknn_model_train = kknn(formula = Spam ~ ., train = train, test = train, k = 30)
kknn_model_test = kknn(formula = Spam ~ ., train = train, test = test, k = 30)

# Train
y_hat_train =
  apply(as.matrix(kknn_model_train$fitted.values), c(1),
        FUN = function(x) return(x > 0.5))
confusion_matrix_kkn_train = as.matrix(table(y_hat_train, train$Spam))
colnames(confusion_matrix_kkn_train) =
  c("Spam", "Normal Mail")
rownames(confusion_matrix_kkn_train) =
  c("Classified as Spam","Classified as no-Spam")

# Test
y_hat_test =
  apply(as.matrix(kknn_model_test$fitted.values), c(1),
        FUN = function(x) return(x > 0.5))
confusion_matrix_kkn_test = as.matrix(table(y_hat_test, test$Spam))
colnames(confusion_matrix_kkn_test) =
  c("Spam", "Normal Mail")
rownames(confusion_matrix_kkn_test) =
  c("Classified as Spam","Classified as no-Spam")
```

Confusion matrices:
```{r, echo = FALSE}
kable(confusion_matrix_kkn_train, caption = "Confusion Matrix (Test Data)")
kable(confusion_matrix_kkn_test, caption = "Confusion Matrix (Test Data)")
```

Result training:
```{r, echo = FALSE}
spam_not_detected_train_kknn =
  confusion_matrix_kkn_train[2,1]/sum(confusion_matrix_kkn_train[,1])
mail_missclassified_train_kknn =
  confusion_matrix_kkn_train[1,2]/sum(confusion_matrix_kkn_train[,2])
print(paste(sep = "", "Spam not detected: ",
            spam_not_detected_train_kknn))
print(paste(sep = "", "Mail missclassified as spam: ",
            mail_missclassified_train_kknn))
print(paste(sep ="", "Misclassification rate: ",
            (confusion_matrix_kkn_train[1,2]+confusion_matrix_kkn_train[2,1])/
              sum(confusion_matrix_kkn_train)))
```

Result test:
```{r, echo = FALSE} 
spam_not_detected_test_kknn =
  confusion_matrix_kkn_test[2,1]/sum(confusion_matrix_kkn_test[,1])
mail_missclassified_test_kknn =
  confusion_matrix_kkn_test[1,2]/sum(confusion_matrix_kkn_test[,2])
print(paste(sep = "", "Spam not detected: ",
            spam_not_detected_test_kknn))
print(paste(sep = "", "Mail missclassified as spam: ",
            mail_missclassified_test_kknn))
print(paste(sep ="", "Misclassification rate: ",
            (confusion_matrix_kkn_test[1,2]+confusion_matrix_kkn_test[2,1])/
              sum(confusion_matrix_kkn_test)))
```

The misclassification rate for kknn for the training set is almost the same. When trying to classifiy on the training data we see, that the misclassification rate almost doubles. It looks like this model is overfitting.

## 1.5)
Train the model with K = 1 and apply it on the training and test data using kknn.

```{r, echo = FALSE}
#### Assigment 1.5 ####
kknn_model_train_1 =
  kknn(formula = Spam ~ ., train = train, test = train, k = 1)
kknn_model_test_1 =
  kknn(formula = Spam ~ ., train = train, test = test, k = 1)

# Train
y_hat_train_1 =
  apply(as.matrix(kknn_model_train_1$fitted.values), c(1),
        FUN = function(x) return(x > 0.5))
confusion_matrix_kkn_train_1 = as.matrix(table(y_hat_train_1, train$Spam))
colnames(confusion_matrix_kkn_train_1) =
  c("Spam", "Normal Mail")
rownames(confusion_matrix_kkn_train_1) =
  c("Classified as Spam","Classified as no-Spam")

# Test
y_hat_test_1 =
  apply(as.matrix(kknn_model_test_1$fitted.values), c(1),
        FUN = function(x) return(x > 0.5))
confusion_matrix_kkn_test_1 = as.matrix(table(y_hat_test_1, test$Spam))
colnames(confusion_matrix_kkn_test_1) =
  c("Spam", "Normal Mail")
rownames(confusion_matrix_kkn_test_1) =
  c("Classified as Spam","Classified as no-Spam")
```

Confusion matrices:
```{r, echo = FALSE}
kable(confusion_matrix_kkn_train_1, caption = "Confusion Matrix (Test Data)")
kable(confusion_matrix_kkn_test_1, caption = "Confusion Matrix (Test Data)")
```

Result training:
```{r, echo = FALSE}
spam_not_detected_train_kknn_1 =
  confusion_matrix_kkn_train_1[2,1]/sum(confusion_matrix_kkn_train_1[,1])
mail_missclassified_train_kknn_1 =
  confusion_matrix_kkn_train_1[1,2]/sum(confusion_matrix_kkn_train_1[,2])
print(paste(sep = "", "Spam not detected: ",
            spam_not_detected_train_kknn_1))
print(paste(sep = "", "Mail missclassified as spam: ",
            mail_missclassified_train_kknn_1))
print(paste(sep ="", "Misclassification rate: ",
        (confusion_matrix_kkn_train_1[1,2]+confusion_matrix_kkn_train_1[2,1])/
          sum(confusion_matrix_kkn_train_1)))
```

Result test:
```{r, echo = FALSE}
spam_not_detected_test_kknn_1 =
  confusion_matrix_kkn_test_1[2,1]/sum(confusion_matrix_kkn_test_1[,1])
mail_missclassified_test_kknn_1 =
  confusion_matrix_kkn_test_1[1,2]/sum(confusion_matrix_kkn_test_1[,2])
print(paste(sep = "", "Spam not detected: ",
            spam_not_detected_test_kknn_1))
print(paste(sep = "", "Mail missclassified as spam: ", 
            mail_missclassified_test_kknn_1))
print(paste(sep ="", "Misclassification rate: ",
          (confusion_matrix_kkn_test_1[1,2]+confusion_matrix_kkn_test_1[2,1])/
            sum(confusion_matrix_kkn_test_1)))
```

We can see that this model is drastically overfitting. Due to k = 1 it's basically learning every point, so it's performing really well on the training set but struggles on the test set.

## Assignment 3 - Feature Selection by Cross-Validation in a Linear Model

We are going to define two functions. The first one is doing the cross validation. We will use this function for the feature selection. You can find them in the appendix.

```{r, echo = FALSE}
#### Assigment 3 ####
c_cross_validation = function(k = 5, Y, X) {

  if (!is.numeric(X) && ncol(X) == 0) {
    y_hat = mean(Y)
    return(mean((y_hat-Y)^2))
  }

  Y = as.matrix(Y)
  X = as.matrix(X)
  X = cbind(1, X)

  # Create a list of 5 matrices with the appropiate
  # size (these will hold the subsets)
  X_subsets = list()
  Y_subsets = list()

  # We fill the list entries with the subsets
  for (i in 1:k) {
    percentage_marker = nrow(X)/k
    start = floor(percentage_marker*(i-1)+1)
    end = floor(percentage_marker*i)
    X_subsets[[i]] = X[start:end,]
    Y_subsets[[i]] = Y[start:end]
  }

  # Now we take one matrix at a time for training and
  # everything else as the testing
  scores = 0
  for (i in 1:k) {

    ## Initial
    X_train = matrix(0, ncol = ncol(X))
    Y_train = c()

    # Get validation and training data
    current_subset_X = X_subsets[-i]
    current_subset_Y = Y_subsets[-i]
    for (j in 1:(length(X_subsets)-1)) {
      X_train = rbind(X_train, current_subset_X[[j]])
      Y_train = c(Y_train, current_subset_Y[[j]])
    }

    # Because of R
    X_train = X_train[-1,]

    # Model
    betas =
      as.matrix((solve(t(X_train) %*% X_train)) %*% t(X_train) %*% Y_train)

    ## Select the training data and transform them to
    # one matrix X_test and one vector Y_test
    X_val = X_subsets[[i]]
    Y_val = Y_subsets[[i]]

    ## Now we get our y_hat and y_real. y_real is
    # only used to clarify the meaning
    y_hat = as.vector(X_val %*% betas)
    y_real = Y_val

    ## Get MSE and add to the scores list
    scores = c(scores, mean((y_hat - y_real)^2))

  }
  # Return the mean of our scores
  scores = scores[-1]
  return(mean(scores))
}

c_best_subset_selection = function(Y, X) {

  # Shuffle X and Y via indexes
  ids = sample(x = 1:nrow(X), nrow(X))
  X = X[ids,]
  Y = Y[ids]

  # Get all combinations
  comb_matrix = matrix(0, ncol = ncol(X))
  for (i in c(1:(2^(ncol(X))-1))) {
    comb_matrix =
      rbind(comb_matrix, tail(rev(as.numeric(intToBits(i))), ncol(X)))
  }

  results = c()

  # Do cross validation for each feature set
  for (j in 1:nrow(comb_matrix)) {
    comb = as.logical(comb_matrix[j,])
    feature_select = X[,comb]
    res = c_cross_validation(5, Y, feature_select)
    results = c(results, res)
  }
  models = matrix(results, ncol = 1)
  models = cbind(models, comb_matrix)

  # Add column with the sum of the features for plotting
  feature_sum = c()
  for (k in 1:nrow(comb_matrix)) {
    row_sum = sum(comb_matrix[k,])
    feature_sum = c(feature_sum, row_sum)
  }
  models = as.data.frame(cbind(feature_sum, models))
  colnames(models)[1:2] = c("Sum", "Score")
  print(ggplot(models, aes(x = Sum, y = Score, colour = factor(feature_sum))) +
    geom_point())
   stat_summary(fun.y = min, colour = "red", geom = "point", size = 5)
  return(models[min(models[,2]) == models[,2],])
}
```

Let's have a look at the results:

```{r, echo = FALSE}
kable(c_best_subset_selection(swiss[,1], swiss[,2:6]))
```

We can see that the range of the score improves with an increasing number of features until a certain threshold, which makes sense as the features give more information but as soon as you select to many features which are unrelated to the predicted value, they a noise and decrease the score again. So in this case we find the optimum with 4 features, as seen in teh plot and the table. The feature that is not selected, is the second one, V4. 

## Assignment 4 - Linear Regression and Regularization

### 4.1)

Let's import the data as well and have a look at it.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
#### Assigment 4.1 ####
tecator_data = read_excel("./tecator.xlsx")
kable(head(tecator_data[,1:10]))
ggplot(tecator_data, aes(x = Protein, y = Moisture)) +
  geom_point() + geom_smooth()

```

A linear model might work for this.

### 4.2)
We know that the Model $M_i$ normally distributed an has an error $\epsilon$:

$$M_i \sim N(\mu,\sigma^2)$$
We know that the expected value is a polynomial function. The polynomal function looks like:

$$E[M_i] = \mu = \beta_0 + \beta_1 * X + \beta_2 * X^2 + ... + \beta_{i-1}*X^{i-1}$$
Let's write this down as a sum:

$$ \mu = \sum_{n=0}^{i-1} \beta_n * X^n$$
If we substite $\mu$ in the formula for the normal distribution, we get our model:

$$M_i \sim N(\sum_{n=0}^{i-1} \beta_n * X^n ,\sigma^2)$$

It's appropiate to use the MSE criterion due the properties that erros are always positive, so we can search for a minimum. Furthermor it highly punishes missclassified data.

### 4.3)

Let's look at the data and plot it.

```{r, echo = FALSE}
#### Assigment 4.3 ####
n = 6
model_tecator_data = data.frame(Y = tecator_data$Moisture)
for (i in c(1:n)) {
  model_tecator_data = data.frame(model_tecator_data, (tecator_data$Protein)^i)
}

for (i in c(1:n)) {
  names(model_tecator_data)[i+1] = paste("Protein", i, sep="")
}

# Shuffle
ids = sample(x = 1:nrow(model_tecator_data), nrow(model_tecator_data))
model_tecator_data = model_tecator_data[ids,]
```

```{r, echo = FALSE}
tecator_data_training =
  model_tecator_data[1:(nrow(model_tecator_data)/2),]
tecator_data_validation =
  model_tecator_data[(nrow(model_tecator_data)/2):nrow(model_tecator_data),]
```

```{r, echo = FALSE}
results = data.frame(Iteration = as.character(),
                  Training_SSE = as.numeric(), Validation_SSE = as.numeric()) 
```

```{r, echo = FALSE}
for (i in c(1:n)) {
  
  formula = "Y ~ Protein1"
  
  if (i != 1) {
    for (j in c(2:i)) {
      formula = paste(formula, " + Protein", j, sep="")
    }
  }
  
  linreg = lm(as.formula(formula), data = tecator_data_training)
  mse_training = mean(linreg$residuals^2)
  y_hat = predict(object = linreg, newdata = tecator_data_validation)
  mse_validation = mean((y_hat - tecator_data_validation$Y)^2)
  results = rbind(results, list(i, mse_training, mse_validation))
}
names(results) = list("Iteration", "MSE_Training", "MSE_Validation")
kable(results)
```

```{r, echo = FALSE, message = FALSE}
ggplot(results) +
  geom_line(aes(x = Iteration, y = MSE_Training, colour = "Training")) +
  geom_line(aes(x = Iteration, y = MSE_Validation, colour = "Validation")) +
  labs(title="MSE vs Iteration", y="MSE", x="Iteration", color = "Legend") +
  scale_color_manual(values = c("blue", "orange"))
```

The best model is the model that performs best on the validation data. In this case it's Model $M_1$. The bias-variance tradeoff is basically a different viewpoint on overfitting. If a model has a high variance but performing well on the training dataset, its probably overfitting. We don't see the exact values of the model here, but we can see that while the model with increasing i is working better on the training dataset, it performs worse on the validation data set. This is exactly due to the mentioned overfitting.

## 4.4)

```{r, echo = FALSE}
#### Assigment 4.4 ####
# Filtering of colums and then just using "Fat ~ ." would also be possible.
c_formula = "Fat ~ Channel1"
for (i in 2:100) {
  c_formula = paste(c_formula, " + Channel", i, sep = "")
}
model = lm (formula = c_formula, data = tecator_data)
model.stepAIC = stepAIC(model, direction = c("both"), trace = FALSE)
summary(model.stepAIC)
```

The model selected 63 channels producing a standard error of 1.107 and having 151 degrees of freedom.

## 4.5)
```{r, echo = FALSE}
#### Assigment 4.5 ####
covariates_ridge = scale(tecator_data[,2:(ncol(tecator_data)-3)])
response_ridge = tecator_data$Fat

glm_model_ridge = glmnet(as.matrix(covariates_ridge),
                         response_ridge, alpha = 0, family="gaussian")
plot(glm_model_ridge, xvar="lambda", label=TRUE)
```

$\lambda$, also called the *Shrinkage Parameter*, penalizes the coefficients to decrease the number of coefficients to prevent overfitting. This is due to the fact that the *Shrinkage Penalty*

$$\lambda  \sum_{j} \beta_{j}^{2}$$

is added to the term of calculating the estimates $\hat{\beta}^{R}$ . If $\lambda = 0$ we're back to least squares estimates.

## 4.6)
```{r, echo = FALSE}
#### Assigment 4.6 ####
covariates_lasso = scale(tecator_data[,2:(ncol(tecator_data)-3)])
response_lasso = tecator_data$Fat

glm_model_lasso = glmnet(as.matrix(covariates_ridge),
                         response_ridge, alpha = 1, family="gaussian")
plot(glm_model_lasso, xvar="lambda", label=TRUE)
```

Ridge Regression has the problem, that even with high $\lambda's$ the coefficients will never be zero, this would only be the case for $l  i  m_{\lambda\to\infty}$. This time the  *Shrinkage Penalty* is

$$ \lambda  \sum_{j} |\beta_{j}|$$
Due to the fact that high $\lambda's$ can set the coefficients to zero, LASSO perform a *Variable Selection*. This is what can bee seen in the plot, with increasing $\ln(\lambda)$ more and more variables are set to 0.

## 4.7
```{r, echo = FALSE}
#### Assigment 4.7 ####
lasso_vc = cv.glmnet(y = response_lasso, x = covariates_lasso,
                     alpha = 1, lambda = seq(from = 0, to = 10, by = 0.01))
plot(lasso_vc)
print(paste(sep = "", "Best lambda score: ", lasso_vc$lambda.min))
```

We see that the best MSE is for $\lambda = l  i  m_{\lambda\to 0} = 0$, so we now that we are performing least squares estimate which includes **all** coefficients.

## 4.8

# Appendix: Source Code

```{r, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```