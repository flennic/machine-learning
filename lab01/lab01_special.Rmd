---
title: "Machine Learning Lab 01 (Special)"
author: "Maximilian Pfundstein (maxpf364)"
date: "11/16/2018"
output:
  html_document:
    df_print: paged
    toc_float: true
  pdf_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(ggplot2)
set.seed(12345)
```

## Special Task 1

### a)
This section includes the function with the k-nearest-neighbour implementation.

```{r}
knearest = function(data, K = 5, newdata) {
  
  d = function(X, Y) {
    
    # Make sure the input in the matrix format
    X = as.matrix(X)
    Y = as.matrix(Y)
    
    # Calculate D as described in the exercise
    X_hat = X/sqrt(rowSums(X^2))
    Y_hat = Y/sqrt(rowSums(Y^2))
    C = X_hat %*% t(Y_hat)
    D = 1 - C
    
    return(D)  
  }
  
  # Get D (trim classification column)
  D = d(data[,-49], newdata[-49])
  
  classification = c()
  
  # For each data entry
  for (i in 1:nrow(D)) {
    # Sort the distances and also get their index
    sortedRow = sort(D[,i], index.return = TRUE)
    
    # Take the K best guys and save their indexed
    indexesKnn = sortedRow$ix[1:K]
    
    # Lookup if they're classified as 0 or 1
    classificationRates = newdata$Spam[indexesKnn]
    
    # Add the classification
    classification = c(classification, (sum(newdata$Spam[indexesKnn]) > (K/2)))
  }
  
  # Look how many got classified correctly
  classifiedCorrectly = sum(classification == newdata$Spam)
  
  # Take "1 -" to get the missclassification rate
  missclassificationRate = 1 - (classifiedCorrectly/nrow(newdata))
  
  return(missclassificationRate)
}
```

### b)
```{r}
# Read the datastet and split it into 50% training and 50% test
spambase = read_excel("spambase.xlsx")
c_data = spambase[1:(nrow(spambase)/2),]
c_newdata = spambase[((nrow(spambase)/2)+1):nrow(spambase),]

# Get the missqualification rates for training and  test
training_rate = knearest(c_data, K = 30, c_data)
test_rate = knearest(c_data, K = 30, c_newdata)

# Print the results
print(training_rate)
print(test_rate)
```


In 1.4) the misclassification rate for the training data set is 17.226% and for the test data set it's 32.993%.

We can see that the custom implementation based on the cosine similarity performs better on the training data compared the the R implemented function. The gap closes when we compare the classification rates on the test dataset, the custom implementation is a little bit worse. This gives the impression, that the custom implementation based on the cosine similarity slightly overfits compared to 1.4.

## Special Task 2
```{r}
density_estimation = function(data, K = 6, X) {
  
  N = length(data)
  S = data
  
  # V needs to be calculated, first get all distances
  distances = abs(X - S)
  
  # Sort them the get the K nearest and get their indexes
  sorted_distances = sort(distances, index.return = TRUE)
  idx_knearest = sorted_distances$ix[1:K]
  
  # The the difference between the highest ans lowest one
  # (to get the volume/distance)
  V = max(data[idx_knearest]) - min(data[idx_knearest])
  
  # Returns based on formula from slides
  return(K/(N*V))
}
```

```{r}
# Get the min and max from the dataset and define a stepsize
min_speed = min(cars$speed)
max_speed = max(cars$speed)
steps = 1000

# X-Values are a sequence from min_speed to max_speed
x_values =   seq(min_speed, max_speed, by = (max_speed - min_speed)/steps)

# Y-Values are the density estimation at each point
y_values = unlist(lapply(x_values,
                         function(x) density_estimation(cars$speed, K = 6, x)))

# Put them into a dataframe
density_data_frame = data.frame(x_values, y_values)
colnames(density_data_frame) = c("X", "Density")

# Plot the data
print(ggplot(density_data_frame) +
  geom_histogram(data = cars, bins = 21) + aes(x = speed, y = ..density..) + 
  geom_line(aes(x = X, y = Density, colour = "Density Function")) +
  labs(title="Density of cars$speed", y="Density", x="X", color = "Legend") +
  scale_color_manual(values = c("orange")))

print(hist(cars$speed), breaks = 10)
```
