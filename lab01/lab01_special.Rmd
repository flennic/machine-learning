---
title: "Machine Learning Lab 01 (Special)"
author: "Maximilian Pfundstein (maxpf364)"
date: "11/16/2018"
output:
  html_document:
    df_print: paged
    toc_float: true
  pdf_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(ggplot2)
library(knitr)
set.seed(12345)
```

## Special Task 1

### a)
This section includes the function with the k-nearest-neighbour implementation.

```{r}
knearest = function(data, K = 5, newdata) {
  
  d = function(X, Y) {
    
    # Make sure the input in the matrix format
    X = as.matrix(X)
    Y = as.matrix(Y)
    
    # Calculate D as described in the exercise
    X_hat = X/sqrt(rowSums(X^2))
    Y_hat = Y/sqrt(rowSums(Y^2))
    C = X_hat %*% t(Y_hat)
    D = 1 - C
    
    return(D)  
  }
  
  # Get D (trim classification column)
  D = d(data[,-49], newdata[-49])
  
  classificationRate = c()
  classification = c()
  classifiedCorrectly = c()
  
  # For each data entry
  for (i in 1:nrow(D)) {
    # Sort the distances and also get their index
    sortedRow = sort(D[,i], index.return = TRUE)
    
    # Take the K best guys and save their indexed
    indexesKnn = sortedRow$ix[1:K]
    
    # Lookup if they're classified as 0 or 1
    classificationRates = newdata$Spam[indexesKnn]
    
    # Add the classification
    classificationRate = c(classificationRate,
                           (sum(newdata$Spam[indexesKnn] / K)))
    temp_classification = sum(newdata$Spam[indexesKnn]) > (K/2)
    classification = c(classification, temp_classification)
    classifiedCorrectly = c(classifiedCorrectly, temp_classification == as.logical(data$Spam[i]))
  }
  
  returnDataFrame = cbind(newdata, classificationRate)
  returnDataFrame = cbind(returnDataFrame, classification)
  returnDataFrame = cbind(returnDataFrame, classifiedCorrectly)
  
  return(returnDataFrame)
}
```

### b)
```{r, echo = FALSE}
# Read the datastet and split it into 50% training and 50% test
spambase = read_excel("spambase.xlsx")
c_data = spambase[1:(nrow(spambase)/2),]
c_newdata = spambase[((nrow(spambase)/2)+1):nrow(spambase),]

# Get the missqualification rates for training and  test
training_classification = knearest(c_data, K = 30, c_data)
test_classification = knearest(c_data, K = 30, c_newdata)

# Get the classification rate
training_rate = 1 - sum(training_classification$classifiedCorrectly/
  nrow(training_classification))
test_rate = 1 - sum(test_classification$classifiedCorrectly/
  nrow(test_classification))
```

In the following the misqualification rates for training and test are shown. The function adds three columns to the data.frame, *classificationRate*, *classification* and *classifiedCorrectly*:

- *classificationRate*: Shows the percentage of neighbours that got classified as valid mail.
- *classification*: Simply checks if $classificationRate > 0.5$ to show the classification.
- *classifiedCorrectly*: Compares the original y and predicted y and tells, if it got classified correctly.


```{r}
# Print the results
kable(head(training_classification[,49:52]))
print(training_rate)
kable(head(test_classification[,49:52]))
print(test_rate)
```


In 1.4) the misclassification rate for the training data set is 17.226% and for the test data set it's 32.993%.

We can see that the custom implementation based on the cosine similarity performs better on the training data compared the the R implemented function. The gap closes when we compare the classification rates on the test dataset, the custom implementation is a little bit worse. This gives the impression, that the custom implementation based on the cosine similarity slightly overfits compared to 1.4.

## Special Task 2

This is the function for the density estimation at point X, K and a given dataset (vector).
```{r}
density_estimation = function(data, K = 6, X) {
  
  N = length(data)
  S = data
  
  # V needs to be calculated, first get all distances
  distances = abs(X - S)
  
  # Sort them the get the K nearest and get their indexes
  sorted_distances = sort(distances, index.return = TRUE)
  idx_knearest = sorted_distances$ix[1:K]
  
  # The the difference between the highest ans lowest one
  # (to get the volume/distance)
  #V = max(data[idx_knearest]) - min(data[idx_knearest])
  V = 2 * max(abs(data[idx_knearest[K]] - X))
  
  # Returns based on formula from slides
  return(K/(N*V))
}
```

Shown is the requested plot, the yellow line shows the density estimation function with 1000 points between min and max of cars$speed. In addition to that the histogram is shown. The number of bins is selected based on aligning to the density function. As the maximum resultion of the speed vector are whole numbers, adding a much higher number of bins doesn't add a lot of more information.

It can be seen, that the estimated density function has some errors in density, but is correlated to the histogram. On average the density is higher (>1 in total which is possible as it is just an esimation and not a real density function). We observe that "peaks" as a density of 0 is not correctly estimated as well as peaks, where the estimated density is much higher.

```{r, echo = FALSE}
# Get the min and max from the dataset and define a stepsize
min_speed = min(cars$speed)
max_speed = max(cars$speed)
steps = 1000

# X-Values are a sequence from min_speed to max_speed
x_values =   seq(min_speed, max_speed, by = (max_speed - min_speed)/steps)

# Y-Values are the density estimation at each point
y_values = unlist(lapply(x_values,
                         function(x) density_estimation(cars$speed, K = 6, x)))

# Put them into a dataframe
density_data_frame = data.frame(x_values, y_values)
colnames(density_data_frame) = c("X", "Density")

# Plot the data
print(ggplot(density_data_frame) +
  geom_histogram(data = cars, bins = 21) + aes(x = speed, y = ..density..) + 
  geom_line(aes(x = X, y = Density, colour = "Density Function")) +
  labs(title="Density of cars$speed", y="Density", x="X", color = "Legend") +
  scale_color_manual(values = c("orange")))
```
